{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae12ed7b",
   "metadata": {},
   "source": [
    "# 工具导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96495647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad38b",
   "metadata": {},
   "source": [
    "# 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef2823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "读取数据集\n",
    "'''\n",
    "test_data = pd.read_csv('./data_format1/test_format1.csv')\n",
    "train_data = pd.read_csv('./data_format1/train_format1.csv')\n",
    "\n",
    "user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "user_log = pd.read_csv('./data_format1/user_log_format1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46d417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163968</td>\n",
       "      <td>4605</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360576</td>\n",
       "      <td>1581</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98688</td>\n",
       "      <td>1964</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98688</td>\n",
       "      <td>3645</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295296</td>\n",
       "      <td>3361</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  prob\n",
       "0   163968         4605   NaN\n",
       "1   360576         1581   NaN\n",
       "2    98688         1964   NaN\n",
       "3    98688         3645   NaN\n",
       "4   295296         3361   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa10d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label\n",
       "0    34176         3906      0\n",
       "1    34176          121      0\n",
       "2    34176         4356      1\n",
       "3    34176         2217      0\n",
       "4   230784         4818      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7feecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376517</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234512</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344532</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186135</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30230</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  age_range  gender\n",
       "0   376517        6.0     1.0\n",
       "1   234512        5.0     0.0\n",
       "2   344532        5.0     0.0\n",
       "3   186135        5.0     0.0\n",
       "4    30230        5.0     0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116e603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328862</td>\n",
       "      <td>323294</td>\n",
       "      <td>833</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>328862</td>\n",
       "      <td>844400</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328862</td>\n",
       "      <td>575153</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328862</td>\n",
       "      <td>996875</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>328862</td>\n",
       "      <td>1086186</td>\n",
       "      <td>1271</td>\n",
       "      <td>1253</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "0   328862   323294     833       2882    2661.0         829            0\n",
       "1   328862   844400    1271       2882    2661.0         829            0\n",
       "2   328862   575153    1271       2882    2661.0         829            0\n",
       "3   328862   996875    1271       2882    2661.0         829            0\n",
       "4   328862  1086186    1271       1253    1049.0         829            0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7604a",
   "metadata": {},
   "source": [
    "# 内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be565cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name, num_rows):\n",
    "    return pd.read_csv(file_name, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df259d21",
   "metadata": {},
   "source": [
    "检测超出机器可表示范围的数值，强行转换类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7949aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7c5ad",
   "metadata": {},
   "source": [
    "# 对数据进行内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e31e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimization is: 32.43 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "num_rows = None\n",
    "num_rows = 200 * 10000 # 1000条测试代码使用\n",
    "\n",
    "train_file = './data_format1/train_format1.csv'\n",
    "test_file = './data_format1/test_format1.csv'\n",
    "user_info_file = './data_format1/user_info_format1.csv'\n",
    "user_log_file = './data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(read_csv(train_file, num_rows))\n",
    "test_data = reduce_mem_usage(read_csv(test_file, num_rows))\n",
    "user_info = reduce_mem_usage(read_csv(user_info_file, num_rows))\n",
    "user_log = reduce_mem_usage(read_csv(user_log_file, num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa52c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3389931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb584e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439c38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 32.4 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba41b7",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 合并用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19352a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender\n",
       "0    34176         3906    0.0        6.0     0.0\n",
       "1    34176          121    0.0        6.0     0.0\n",
       "2    34176         4356    1.0        6.0     0.0\n",
       "3    34176         2217    0.0        6.0     0.0\n",
       "4   230784         4818    0.0        0.0     0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_data['prob']\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left')\n",
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6411b038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 释放内存\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481544c",
   "metadata": {},
   "source": [
    "## 将用户行为日志根据时间进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12955ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61975</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61976</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61977</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61978</th>\n",
       "      <td>16</td>\n",
       "      <td>962763</td>\n",
       "      <td>19</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61979</th>\n",
       "      <td>16</td>\n",
       "      <td>391126</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541446</th>\n",
       "      <td>424164</td>\n",
       "      <td>61016</td>\n",
       "      <td>737</td>\n",
       "      <td>859</td>\n",
       "      <td>3724.0</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541447</th>\n",
       "      <td>424164</td>\n",
       "      <td>72017</td>\n",
       "      <td>662</td>\n",
       "      <td>606</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541448</th>\n",
       "      <td>424164</td>\n",
       "      <td>125913</td>\n",
       "      <td>1577</td>\n",
       "      <td>606</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541449</th>\n",
       "      <td>424164</td>\n",
       "      <td>20716</td>\n",
       "      <td>1238</td>\n",
       "      <td>606</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541450</th>\n",
       "      <td>424164</td>\n",
       "      <td>516472</td>\n",
       "      <td>1238</td>\n",
       "      <td>2468</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "61975        16   980982     437        650    4276.0         914            0\n",
       "61976        16   980982     437        650    4276.0         914            0\n",
       "61977        16   980982     437        650    4276.0         914            0\n",
       "61978        16   962763      19        650    4276.0         914            0\n",
       "61979        16   391126     437        650    4276.0         914            0\n",
       "...         ...      ...     ...        ...       ...         ...          ...\n",
       "541446   424164    61016     737        859    3724.0        1111            0\n",
       "541447   424164    72017     662        606     376.0        1111            0\n",
       "541448   424164   125913    1577        606     376.0        1111            0\n",
       "541449   424164    20716    1238        606     376.0        1111            0\n",
       "541450   424164   516472    1238       2468    1392.0        1111            0\n",
       "\n",
       "[2000000 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log = user_log.sort_values(['user_id','time_stamp'])\n",
    "user_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e3fd2",
   "metadata": {},
   "source": [
    "## 根据用户id分组，汇总Item_id,cat_id,seller_id,brand_id,time_stamp,action_type字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fb6c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并函数\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "new_column = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "}\n",
    "\n",
    "user_log_path = user_log.groupby('user_id').agg(agg_dict).reset_index().rename(columns=new_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4396f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>980982 980982 980982 962763 391126 827174 6731...</td>\n",
       "      <td>437 437 437 19 437 437 437 437 895 19 437 437 ...</td>\n",
       "      <td>650 650 650 650 650 650 650 650 3948 650 650 6...</td>\n",
       "      <td>4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...</td>\n",
       "      <td>914 914 914 914 914 914 914 914 914 914 914 91...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>388018 388018 88673 88673 88673 88673 846066 5...</td>\n",
       "      <td>949 949 614 614 614 614 420 1401 948 948 513 1...</td>\n",
       "      <td>2772 2772 4066 4066 4066 4066 4951 4951 2872 2...</td>\n",
       "      <td>2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...</td>\n",
       "      <td>710 710 711 711 711 711 908 908 1105 1105 1105...</td>\n",
       "      <td>0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>60215 1004605 60215 60215 60215 60215 628525 5...</td>\n",
       "      <td>1308 1308 1308 1308 1308 1308 1271 656 656 656...</td>\n",
       "      <td>2128 3207 2128 2128 2128 2128 3142 4618 4618 4...</td>\n",
       "      <td>3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...</td>\n",
       "      <td>521 521 521 521 521 522 529 828 828 828 828 82...</td>\n",
       "      <td>0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>889499 528459 765746 553259 889499 22435 40047...</td>\n",
       "      <td>662 1075 662 1577 662 11 184 1604 11 11 177 11...</td>\n",
       "      <td>4048 601 3104 3828 4048 4766 2419 2768 2565 26...</td>\n",
       "      <td>5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...</td>\n",
       "      <td>517 520 525 528 602 602 610 610 610 610 610 61...</td>\n",
       "      <td>3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>979639 890128 981780 211366 211366 797946 4567...</td>\n",
       "      <td>267 1271 1505 267 267 1075 1075 407 407 1075 4...</td>\n",
       "      <td>2429 4785 3784 800 800 1595 1418 2662 2662 315...</td>\n",
       "      <td>2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...</td>\n",
       "      <td>529 529 602 604 604 607 607 607 607 607 607 60...</td>\n",
       "      <td>0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          item_path  \\\n",
       "0       16  980982 980982 980982 962763 391126 827174 6731...   \n",
       "1       19  388018 388018 88673 88673 88673 88673 846066 5...   \n",
       "2       41  60215 1004605 60215 60215 60215 60215 628525 5...   \n",
       "3       56  889499 528459 765746 553259 889499 22435 40047...   \n",
       "4      155  979639 890128 981780 211366 211366 797946 4567...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  437 437 437 19 437 437 437 437 895 19 437 437 ...   \n",
       "1  949 949 614 614 614 614 420 1401 948 948 513 1...   \n",
       "2  1308 1308 1308 1308 1308 1308 1271 656 656 656...   \n",
       "3  662 1075 662 1577 662 11 184 1604 11 11 177 11...   \n",
       "4  267 1271 1505 267 267 1075 1075 407 407 1075 4...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  650 650 650 650 650 650 650 650 3948 650 650 6...   \n",
       "1  2772 2772 4066 4066 4066 4066 4951 4951 2872 2...   \n",
       "2  2128 3207 2128 2128 2128 2128 3142 4618 4618 4...   \n",
       "3  4048 601 3104 3828 4048 4766 2419 2768 2565 26...   \n",
       "4  2429 4785 3784 800 800 1595 1418 2662 2662 315...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...   \n",
       "1  2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...   \n",
       "2  3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...   \n",
       "3  5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...   \n",
       "4  2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  914 914 914 914 914 914 914 914 914 914 914 91...   \n",
       "1  710 710 711 711 711 711 908 908 1105 1105 1105...   \n",
       "2  521 521 521 521 521 522 529 828 828 828 828 82...   \n",
       "3  517 520 525 528 602 602 610 610 610 610 610 61...   \n",
       "4  529 529 602 604 604 607 607 607 607 607 607 60...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...  \n",
       "1  0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "3  3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "4  0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_path.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed35bf9",
   "metadata": {},
   "source": [
    "## 合并用户label信息和用户行为信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c91bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = all_data.merge(user_log_path,on='user_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10aeeba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1a21c",
   "metadata": {},
   "source": [
    "## 删除数据并回收内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f41077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_log数据不再需要，释放内存\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d91161",
   "metadata": {},
   "source": [
    "# 定义数据统计函数：\n",
    "## 1. 统计数据总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3183db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0032a76",
   "metadata": {},
   "source": [
    "## 2. 统计唯一数据总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a65b47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7e8bd",
   "metadata": {},
   "source": [
    "## 3. 统计数据最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c15e7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c25e83",
   "metadata": {},
   "source": [
    "## 4. 统计数据最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d113419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cf17c",
   "metadata": {},
   "source": [
    "## 5. 统计数据的标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5623fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06187475",
   "metadata": {},
   "source": [
    "## 6. 统计数据中top N的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d309c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n(x,n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35731",
   "metadata": {},
   "source": [
    "## 7. 统计数据中top N的数据的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb71db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n_cnt(x,n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c240eba",
   "metadata": {},
   "source": [
    "## 8. 函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb493d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d74523",
   "metadata": {},
   "source": [
    "# 提取商铺的基本统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6cb2264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test = all_data_path.head(2000)\n",
    "all_data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "937c8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计用户点击总次数\n",
    "all_data_test = user_cnt(all_data_test,'action_type_path','user_cnt')\n",
    "# 统计不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 统计不同商品品类的个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 统计不同品牌的个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 统计不同商品的数量\n",
    "all_data_test = user_nunique(all_data_test,'item_path','item_nunique')\n",
    "# 统计每个用户的活跃天数\n",
    "all_data_test = user_nunique(all_data_test,'time_stamp_path','time_stamp_nunique')\n",
    "# 统计每个用户不同行为种类数\n",
    "all_data_test = user_nunique(all_data_test,'action_type_path','action_type_nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b79a093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>action_type_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>310</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>217</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "      <td>70</td>\n",
       "      <td>159</td>\n",
       "      <td>233</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>278</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>148</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  user_cnt  \\\n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       310   \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...       274   \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       278   \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "\n",
       "   seller_nunique  cat_nunique  brand_nunique  item_nunique  \\\n",
       "0              96           37             88           217   \n",
       "1             181           70            159           233   \n",
       "2              57           59             62           148   \n",
       "3              49           35             45           170   \n",
       "4              49           35             45           170   \n",
       "\n",
       "   time_stamp_nunique  action_type_nunique  \n",
       "0                  29                    2  \n",
       "1                  52                    3  \n",
       "2                  35                    3  \n",
       "3                   9                    2  \n",
       "4                   9                    2  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "313c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,'time_stamp_path','time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,'time_stamp_path','time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,'time_stamp_path','time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b170b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test,'seller_path','seller_most_1',n=1)\n",
    "# 每个用户最喜欢的商品品类\n",
    "all_data_test = user_most_n(all_data_test,'cat_path','cat_most_1',n=1)\n",
    "# 每个用户最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test,'brand_path','brand_most_1',n=1)\n",
    "# 每个用户最频繁的行为\n",
    "all_data_test = user_most_n(all_data_test,'action_type_path','action_type_1',n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2868c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个用户最喜欢的店铺 行为总次数\n",
    "all_data_test = user_most_n_cnt(all_data_test,'seller_path','seller_most_1_cnt',n=1)\n",
    "# 每个用户最喜欢的商品品类 行为总次数\n",
    "all_data_test = user_most_n_cnt(all_data_test,'cat_path','cat_most_1_cnt',n=1)\n",
    "# 每个用户最喜欢的品牌 行为总次数\n",
    "all_data_test = user_most_n_cnt(all_data_test,'brand_path','brand_most_1_cnt',n=1)\n",
    "# 每个用户最频繁的行为 行为总次数\n",
    "all_data_test = user_most_n_cnt(all_data_test,'action_type_path','action_type_1_cnt',n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "431a291b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>time_stamp_std</th>\n",
       "      <th>time_stamp_range</th>\n",
       "      <th>seller_most_1</th>\n",
       "      <th>cat_most_1</th>\n",
       "      <th>brand_most_1</th>\n",
       "      <th>action_type_1</th>\n",
       "      <th>seller_most_1_cnt</th>\n",
       "      <th>cat_most_1_cnt</th>\n",
       "      <th>brand_most_1_cnt</th>\n",
       "      <th>action_type_1_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>196.143254</td>\n",
       "      <td>593</td>\n",
       "      <td>1704</td>\n",
       "      <td>629</td>\n",
       "      <td>5580.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>188.604871</td>\n",
       "      <td>594</td>\n",
       "      <td>3645</td>\n",
       "      <td>662</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>145.929386</td>\n",
       "      <td>594</td>\n",
       "      <td>1369</td>\n",
       "      <td>1213</td>\n",
       "      <td>3332.0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>53.067342</td>\n",
       "      <td>187</td>\n",
       "      <td>361</td>\n",
       "      <td>602</td>\n",
       "      <td>5736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>53.067342</td>\n",
       "      <td>187</td>\n",
       "      <td>361</td>\n",
       "      <td>602</td>\n",
       "      <td>5736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  ... time_stamp_std  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...  ...     196.143254   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...  ...     188.604871   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...  ...     145.929386   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...  ...      53.067342   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...  ...      53.067342   \n",
       "\n",
       "   time_stamp_range  seller_most_1  cat_most_1  brand_most_1  action_type_1  \\\n",
       "0               593           1704         629        5580.0              0   \n",
       "1               594           3645         662        3928.0              0   \n",
       "2               594           1369        1213        3332.0              0   \n",
       "3               187            361         602        5736.0              0   \n",
       "4               187            361         602        5736.0              0   \n",
       "\n",
       "   seller_most_1_cnt  cat_most_1_cnt  brand_most_1_cnt  action_type_1_cnt  \n",
       "0                 35              43                35                299  \n",
       "1                  9              56                11                259  \n",
       "2                 93              29                48                241  \n",
       "3                 45              68                45                228  \n",
       "4                 45              68                45                228  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb3ee8",
   "metadata": {},
   "source": [
    "# 分别统计用户的点击、加购、购买、收藏的特征\n",
    "## 不同行为的业务函数定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9effa4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计符合action_type的行为总数\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(data_out)  \n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计符合action_type的非重复行为次数\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# 封装函数\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c92db1",
   "metadata": {},
   "source": [
    "## 统计店铺被用户点击的次数、加购的次数、购买的次数、收藏的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c134afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "# 不同店铺点击次数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# 不同店铺加购次数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '1', 'seller_nunique_1')\n",
    "# 不同店铺购买次数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '2', 'seller_nunique_2')\n",
    "# 不同店铺收藏次数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '3', 'seller_nunique_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c37df65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_most_1_cnt</th>\n",
       "      <th>action_type_1_cnt</th>\n",
       "      <th>user_cnt_0</th>\n",
       "      <th>user_cnt_1</th>\n",
       "      <th>user_cnt_2</th>\n",
       "      <th>user_cnt_3</th>\n",
       "      <th>seller_nunique_0</th>\n",
       "      <th>seller_nunique_1</th>\n",
       "      <th>seller_nunique_2</th>\n",
       "      <th>seller_nunique_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>299</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>241</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  ... brand_most_1_cnt  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...  ...               35   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...  ...               11   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...  ...               48   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...  ...               45   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...  ...               45   \n",
       "\n",
       "   action_type_1_cnt  user_cnt_0  user_cnt_1  user_cnt_2  user_cnt_3  \\\n",
       "0                299         310         310         310         310   \n",
       "1                259         274         274         274         274   \n",
       "2                241         278         278         278         278   \n",
       "3                228         237         237         237         237   \n",
       "4                228         237         237         237         237   \n",
       "\n",
       "   seller_nunique_0  seller_nunique_1  seller_nunique_2  seller_nunique_3  \n",
       "0                97                 1                 9                 1  \n",
       "1               181                 1                 9                 6  \n",
       "2                56                 1                 9                 7  \n",
       "3                50                 1                 6                 1  \n",
       "4                50                 1                 6                 1  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830da7dd",
   "metadata": {},
   "source": [
    "# 组合特征\n",
    "## 查看提取的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d4c97c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'merchant_id',\n",
       " 'label',\n",
       " 'age_range',\n",
       " 'gender',\n",
       " 'item_path',\n",
       " 'cat_path',\n",
       " 'seller_path',\n",
       " 'brand_path',\n",
       " 'time_stamp_path',\n",
       " 'action_type_path',\n",
       " 'user_cnt',\n",
       " 'seller_nunique',\n",
       " 'cat_nunique',\n",
       " 'brand_nunique',\n",
       " 'item_nunique',\n",
       " 'time_stamp_nunique',\n",
       " 'action_type_nunique',\n",
       " 'time_stamp_max',\n",
       " 'time_stamp_min',\n",
       " 'time_stamp_std',\n",
       " 'time_stamp_range',\n",
       " 'seller_most_1',\n",
       " 'cat_most_1',\n",
       " 'brand_most_1',\n",
       " 'action_type_1',\n",
       " 'seller_most_1_cnt',\n",
       " 'cat_most_1_cnt',\n",
       " 'brand_most_1_cnt',\n",
       " 'action_type_1_cnt',\n",
       " 'user_cnt_0',\n",
       " 'user_cnt_1',\n",
       " 'user_cnt_2',\n",
       " 'user_cnt_3',\n",
       " 'seller_nunique_0',\n",
       " 'seller_nunique_1',\n",
       " 'seller_nunique_2',\n",
       " 'seller_nunique_3']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdab04",
   "metadata": {},
   "source": [
    "# 利用countvector，tfidf提取特征\n",
    "Refercence:https://www.cnblogs.com/nxf-rabbit75/p/9353212.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c0f01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    all_data_test[col] = all_data_test[col].astype(str)\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afb986",
   "metadata": {},
   "source": [
    "## 特征重命名&合并——原数据+TF-IDF文本特征提取结果 \n",
    "### (2000 Instances × 100 words）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f414ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58f3d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_90</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  ... tfidf_90  tfidf_91  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...  ...      0.0  0.000000   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...  ...      0.0  0.151756   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...  ...      0.0  0.000000   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...  ...      0.0  0.000000   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...  ...      0.0  0.000000   \n",
       "\n",
       "   tfidf_92  tfidf_93  tfidf_94  tfidf_95  tfidf_96  tfidf_97  tfidf_98  \\\n",
       "0       0.0  0.115594  0.000000       0.0  0.000000       0.0  0.000000   \n",
       "1       0.0  0.000000  0.438598       0.0  0.163503       0.0  0.000000   \n",
       "2       0.0  0.000000  0.000000       0.0  0.000000       0.0  0.203564   \n",
       "3       0.0  0.000000  0.053659       0.0  0.015003       0.0  0.000000   \n",
       "4       0.0  0.000000  0.053659       0.0  0.015003       0.0  0.000000   \n",
       "\n",
       "   tfidf_99  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a2248",
   "metadata": {},
   "source": [
    "# embeeding特征\n",
    "#### 离散数字特征——>100维向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbc674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), window=5, min_count=5, workers=4)\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ebfc0c",
   "metadata": {},
   "source": [
    "## embeeding特征和原始特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea070440",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test,df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116f8db",
   "metadata": {},
   "source": [
    "# Stacking特征\n",
    "Reference:https://www.youtube.com/watch?v=lcXKFS65BI0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "303a7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "809adbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c66c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c140c",
   "metadata": {},
   "source": [
    "## 获取训练和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73d44bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fac566",
   "metadata": {},
   "source": [
    "## 处理函数值Inf以及nan的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b403eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0112cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b256663",
   "metadata": {},
   "source": [
    "## 导入划分数据函数（stacking原始训练数据folder=5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efc9219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a974dd7",
   "metadata": {},
   "source": [
    "## 使用lgb和xgb分类模型构造stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9554e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2224850",
   "metadata": {},
   "source": [
    "## 训练模型，获取stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfd82f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7302\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 130\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[1]\tvalid_0's multi_logloss: 0.294257\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.294024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.293904\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.293788\n",
      "[5]\tvalid_0's multi_logloss: 0.293497\n",
      "[6]\tvalid_0's multi_logloss: 0.293532\n",
      "[7]\tvalid_0's multi_logloss: 0.293465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.293354\n",
      "[9]\tvalid_0's multi_logloss: 0.293289\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.293341\n",
      "[11]\tvalid_0's multi_logloss: 0.293093\n",
      "[12]\tvalid_0's multi_logloss: 0.293153\n",
      "[13]\tvalid_0's multi_logloss: 0.292745\n",
      "[14]\tvalid_0's multi_logloss: 0.292656\n",
      "[15]\tvalid_0's multi_logloss: 0.292593\n",
      "[16]\tvalid_0's multi_logloss: 0.292552\n",
      "[17]\tvalid_0's multi_logloss: 0.292576\n",
      "[18]\tvalid_0's multi_logloss: 0.292465\n",
      "[19]\tvalid_0's multi_logloss: 0.292394\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.29244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.292623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.292679\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.292957\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.293198\n",
      "[25]\tvalid_0's multi_logloss: 0.293494\n",
      "[26]\tvalid_0's multi_logloss: 0.293652\n",
      "[27]\tvalid_0's multi_logloss: 0.293842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.294037\n",
      "[29]\tvalid_0's multi_logloss: 0.294123\n",
      "[30]\tvalid_0's multi_logloss: 0.294092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.294009\n",
      "[32]\tvalid_0's multi_logloss: 0.294497\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's multi_logloss: 0.294632\n",
      "[34]\tvalid_0's multi_logloss: 0.294919\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.295224\n",
      "[36]\tvalid_0's multi_logloss: 0.295374\n",
      "[37]\tvalid_0's multi_logloss: 0.295642\n",
      "[38]\tvalid_0's multi_logloss: 0.295865\n",
      "[39]\tvalid_0's multi_logloss: 0.296103\n",
      "[40]\tvalid_0's multi_logloss: 0.296458\n",
      "[41]\tvalid_0's multi_logloss: 0.296949\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.29703\n",
      "[43]\tvalid_0's multi_logloss: 0.297394\n",
      "[44]\tvalid_0's multi_logloss: 0.297758\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.297842\n",
      "[46]\tvalid_0's multi_logloss: 0.29807\n",
      "[47]\tvalid_0's multi_logloss: 0.298065\n",
      "[48]\tvalid_0's multi_logloss: 0.298209\n",
      "[49]\tvalid_0's multi_logloss: 0.29822\n",
      "[50]\tvalid_0's multi_logloss: 0.298485\n",
      "[51]\tvalid_0's multi_logloss: 0.298577\n",
      "[52]\tvalid_0's multi_logloss: 0.298519\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.29876\n",
      "[54]\tvalid_0's multi_logloss: 0.298871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.298926\n",
      "[56]\tvalid_0's multi_logloss: 0.299228\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.299607\n",
      "[58]\tvalid_0's multi_logloss: 0.299823\n",
      "[59]\tvalid_0's multi_logloss: 0.29994\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.299957\n",
      "[61]\tvalid_0's multi_logloss: 0.30007\n",
      "[62]\tvalid_0's multi_logloss: 0.299901\n",
      "[63]\tvalid_0's multi_logloss: 0.299726\n",
      "[64]\tvalid_0's multi_logloss: 0.299963\n",
      "[65]\tvalid_0's multi_logloss: 0.299926\n",
      "[66]\tvalid_0's multi_logloss: 0.299803\n",
      "[67]\tvalid_0's multi_logloss: 0.299955\n",
      "[68]\tvalid_0's multi_logloss: 0.299957\n",
      "[69]\tvalid_0's multi_logloss: 0.299811\n",
      "[70]\tvalid_0's multi_logloss: 0.299985\n",
      "[71]\tvalid_0's multi_logloss: 0.299791\n",
      "[72]\tvalid_0's multi_logloss: 0.2996\n",
      "[73]\tvalid_0's multi_logloss: 0.299694\n",
      "[74]\tvalid_0's multi_logloss: 0.299789\n",
      "[75]\tvalid_0's multi_logloss: 0.300065\n",
      "[76]\tvalid_0's multi_logloss: 0.300026\n",
      "[77]\tvalid_0's multi_logloss: 0.300057\n",
      "[78]\tvalid_0's multi_logloss: 0.300084\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.300063\n",
      "[80]\tvalid_0's multi_logloss: 0.300202\n",
      "[81]\tvalid_0's multi_logloss: 0.300486\n",
      "[82]\tvalid_0's multi_logloss: 0.300526\n",
      "[83]\tvalid_0's multi_logloss: 0.300859\n",
      "[84]\tvalid_0's multi_logloss: 0.300966\n",
      "[85]\tvalid_0's multi_logloss: 0.301025\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.301264\n",
      "[87]\tvalid_0's multi_logloss: 0.301587\n",
      "[88]\tvalid_0's multi_logloss: 0.301652\n",
      "[89]\tvalid_0's multi_logloss: 0.301881\n",
      "[90]\tvalid_0's multi_logloss: 0.30197\n",
      "[91]\tvalid_0's multi_logloss: 0.30217\n",
      "[92]\tvalid_0's multi_logloss: 0.302478\n",
      "[93]\tvalid_0's multi_logloss: 0.3026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's multi_logloss: 0.302791\n",
      "[95]\tvalid_0's multi_logloss: 0.302747\n",
      "[96]\tvalid_0's multi_logloss: 0.302733\n",
      "[97]\tvalid_0's multi_logloss: 0.302957\n",
      "[98]\tvalid_0's multi_logloss: 0.303156\n",
      "[99]\tvalid_0's multi_logloss: 0.303137\n",
      "[100]\tvalid_0's multi_logloss: 0.30329\n",
      "[101]\tvalid_0's multi_logloss: 0.303426\n",
      "[102]\tvalid_0's multi_logloss: 0.303753\n",
      "[103]\tvalid_0's multi_logloss: 0.303784\n",
      "[104]\tvalid_0's multi_logloss: 0.303999\n",
      "[105]\tvalid_0's multi_logloss: 0.304346\n",
      "[106]\tvalid_0's multi_logloss: 0.304595\n",
      "[107]\tvalid_0's multi_logloss: 0.304905\n",
      "[108]\tvalid_0's multi_logloss: 0.305113\n",
      "[109]\tvalid_0's multi_logloss: 0.305198\n",
      "[110]\tvalid_0's multi_logloss: 0.30542\n",
      "[111]\tvalid_0's multi_logloss: 0.305608\n",
      "[112]\tvalid_0's multi_logloss: 0.305799\n",
      "[113]\tvalid_0's multi_logloss: 0.30591\n",
      "[114]\tvalid_0's multi_logloss: 0.305997\n",
      "[115]\tvalid_0's multi_logloss: 0.306261\n",
      "[116]\tvalid_0's multi_logloss: 0.306278\n",
      "[117]\tvalid_0's multi_logloss: 0.3064\n",
      "[118]\tvalid_0's multi_logloss: 0.30648\n",
      "[119]\tvalid_0's multi_logloss: 0.306895\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_logloss: 0.292394\n",
      "lgb now score is: [2.5873191651232696]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7338\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 130\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.220809\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.220539\n",
      "[3]\tvalid_0's multi_logloss: 0.220263\n",
      "[4]\tvalid_0's multi_logloss: 0.220243\n",
      "[5]\tvalid_0's multi_logloss: 0.220127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.220005\n",
      "[7]\tvalid_0's multi_logloss: 0.219678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\tvalid_0's multi_logloss: 0.219223\n",
      "[9]\tvalid_0's multi_logloss: 0.219039\n",
      "[10]\tvalid_0's multi_logloss: 0.218863\n",
      "[11]\tvalid_0's multi_logloss: 0.218607\n",
      "[12]\tvalid_0's multi_logloss: 0.218579\n",
      "[13]\tvalid_0's multi_logloss: 0.218359\n",
      "[14]\tvalid_0's multi_logloss: 0.21808\n",
      "[15]\tvalid_0's multi_logloss: 0.218005\n",
      "[16]\tvalid_0's multi_logloss: 0.217997\n",
      "[17]\tvalid_0's multi_logloss: 0.217931\n",
      "[18]\tvalid_0's multi_logloss: 0.217667\n",
      "[19]\tvalid_0's multi_logloss: 0.217793\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.217735\n",
      "[21]\tvalid_0's multi_logloss: 0.217838\n",
      "[22]\tvalid_0's multi_logloss: 0.217838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.217279\n",
      "[24]\tvalid_0's multi_logloss: 0.217145\n",
      "[25]\tvalid_0's multi_logloss: 0.21713\n",
      "[26]\tvalid_0's multi_logloss: 0.21723\n",
      "[27]\tvalid_0's multi_logloss: 0.21743\n",
      "[28]\tvalid_0's multi_logloss: 0.217448\n",
      "[29]\tvalid_0's multi_logloss: 0.217584\n",
      "[30]\tvalid_0's multi_logloss: 0.217565\n",
      "[31]\tvalid_0's multi_logloss: 0.217577\n",
      "[32]\tvalid_0's multi_logloss: 0.217764\n",
      "[33]\tvalid_0's multi_logloss: 0.217992\n",
      "[34]\tvalid_0's multi_logloss: 0.218093\n",
      "[35]\tvalid_0's multi_logloss: 0.218177\n",
      "[36]\tvalid_0's multi_logloss: 0.218192\n",
      "[37]\tvalid_0's multi_logloss: 0.218175\n",
      "[38]\tvalid_0's multi_logloss: 0.218282\n",
      "[39]\tvalid_0's multi_logloss: 0.218334\n",
      "[40]\tvalid_0's multi_logloss: 0.218525\n",
      "[41]\tvalid_0's multi_logloss: 0.218889\n",
      "[42]\tvalid_0's multi_logloss: 0.218904\n",
      "[43]\tvalid_0's multi_logloss: 0.219043\n",
      "[44]\tvalid_0's multi_logloss: 0.219197\n",
      "[45]\tvalid_0's multi_logloss: 0.219276\n",
      "[46]\tvalid_0's multi_logloss: 0.219316\n",
      "[47]\tvalid_0's multi_logloss: 0.219439\n",
      "[48]\tvalid_0's multi_logloss: 0.219324\n",
      "[49]\tvalid_0's multi_logloss: 0.219386\n",
      "[50]\tvalid_0's multi_logloss: 0.219276\n",
      "[51]\tvalid_0's multi_logloss: 0.219424\n",
      "[52]\tvalid_0's multi_logloss: 0.219388\n",
      "[53]\tvalid_0's multi_logloss: 0.219611\n",
      "[54]\tvalid_0's multi_logloss: 0.219633\n",
      "[55]\tvalid_0's multi_logloss: 0.219735\n",
      "[56]\tvalid_0's multi_logloss: 0.219692\n",
      "[57]\tvalid_0's multi_logloss: 0.219911\n",
      "[58]\tvalid_0's multi_logloss: 0.21999\n",
      "[59]\tvalid_0's multi_logloss: 0.219875\n",
      "[60]\tvalid_0's multi_logloss: 0.220002\n",
      "[61]\tvalid_0's multi_logloss: 0.220174\n",
      "[62]\tvalid_0's multi_logloss: 0.220099\n",
      "[63]\tvalid_0's multi_logloss: 0.220227\n",
      "[64]\tvalid_0's multi_logloss: 0.220155\n",
      "[65]\tvalid_0's multi_logloss: 0.220273\n",
      "[66]\tvalid_0's multi_logloss: 0.220468\n",
      "[67]\tvalid_0's multi_logloss: 0.220529\n",
      "[68]\tvalid_0's multi_logloss: 0.220562\n",
      "[69]\tvalid_0's multi_logloss: 0.220852\n",
      "[70]\tvalid_0's multi_logloss: 0.220851\n",
      "[71]\tvalid_0's multi_logloss: 0.220922\n",
      "[72]\tvalid_0's multi_logloss: 0.221214\n",
      "[73]\tvalid_0's multi_logloss: 0.221333\n",
      "[74]\tvalid_0's multi_logloss: 0.221378\n",
      "[75]\tvalid_0's multi_logloss: 0.221526\n",
      "[76]\tvalid_0's multi_logloss: 0.221354\n",
      "[77]\tvalid_0's multi_logloss: 0.221622\n",
      "[78]\tvalid_0's multi_logloss: 0.221821\n",
      "[79]\tvalid_0's multi_logloss: 0.222053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's multi_logloss: 0.222106\n",
      "[81]\tvalid_0's multi_logloss: 0.222239\n",
      "[82]\tvalid_0's multi_logloss: 0.222172\n",
      "[83]\tvalid_0's multi_logloss: 0.222288\n",
      "[84]\tvalid_0's multi_logloss: 0.222425\n",
      "[85]\tvalid_0's multi_logloss: 0.222512\n",
      "[86]\tvalid_0's multi_logloss: 0.222569\n",
      "[87]\tvalid_0's multi_logloss: 0.222655\n",
      "[88]\tvalid_0's multi_logloss: 0.22294\n",
      "[89]\tvalid_0's multi_logloss: 0.223017\n",
      "[90]\tvalid_0's multi_logloss: 0.223103\n",
      "[91]\tvalid_0's multi_logloss: 0.223322\n",
      "[92]\tvalid_0's multi_logloss: 0.223335\n",
      "[93]\tvalid_0's multi_logloss: 0.223417\n",
      "[94]\tvalid_0's multi_logloss: 0.223505\n",
      "[95]\tvalid_0's multi_logloss: 0.223595\n",
      "[96]\tvalid_0's multi_logloss: 0.223792\n",
      "[97]\tvalid_0's multi_logloss: 0.223812\n",
      "[98]\tvalid_0's multi_logloss: 0.223969\n",
      "[99]\tvalid_0's multi_logloss: 0.224179\n",
      "[100]\tvalid_0's multi_logloss: 0.224174\n",
      "[101]\tvalid_0's multi_logloss: 0.224172\n",
      "[102]\tvalid_0's multi_logloss: 0.224448\n",
      "[103]\tvalid_0's multi_logloss: 0.224347\n",
      "[104]\tvalid_0's multi_logloss: 0.22442\n",
      "[105]\tvalid_0's multi_logloss: 0.224492\n",
      "[106]\tvalid_0's multi_logloss: 0.224431\n",
      "[107]\tvalid_0's multi_logloss: 0.224316\n",
      "[108]\tvalid_0's multi_logloss: 0.224197\n",
      "[109]\tvalid_0's multi_logloss: 0.224484\n",
      "[110]\tvalid_0's multi_logloss: 0.224369\n",
      "[111]\tvalid_0's multi_logloss: 0.224362\n",
      "[112]\tvalid_0's multi_logloss: 0.224487\n",
      "[113]\tvalid_0's multi_logloss: 0.224353\n",
      "[114]\tvalid_0's multi_logloss: 0.224498\n",
      "[115]\tvalid_0's multi_logloss: 0.224689\n",
      "[116]\tvalid_0's multi_logloss: 0.224963\n",
      "[117]\tvalid_0's multi_logloss: 0.225049\n",
      "[118]\tvalid_0's multi_logloss: 0.225145\n",
      "[119]\tvalid_0's multi_logloss: 0.225455\n",
      "[120]\tvalid_0's multi_logloss: 0.225385\n",
      "[121]\tvalid_0's multi_logloss: 0.225439\n",
      "[122]\tvalid_0's multi_logloss: 0.225348\n",
      "[123]\tvalid_0's multi_logloss: 0.225406\n",
      "[124]\tvalid_0's multi_logloss: 0.225479\n",
      "[125]\tvalid_0's multi_logloss: 0.2257\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's multi_logloss: 0.21713\n",
      "lgb now score is: [2.5873191651232696, 2.595481008646282]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 130\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "[1]\tvalid_0's multi_logloss: 0.221573\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.221773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.222053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.222149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.222446\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.222595\n",
      "[7]\tvalid_0's multi_logloss: 0.222764\n",
      "[8]\tvalid_0's multi_logloss: 0.223204\n",
      "[9]\tvalid_0's multi_logloss: 0.223497\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.223799\n",
      "[11]\tvalid_0's multi_logloss: 0.224265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.224687\n",
      "[13]\tvalid_0's multi_logloss: 0.224963\n",
      "[14]\tvalid_0's multi_logloss: 0.225095\n",
      "[15]\tvalid_0's multi_logloss: 0.225195\n",
      "[16]\tvalid_0's multi_logloss: 0.22562\n",
      "[17]\tvalid_0's multi_logloss: 0.225748\n",
      "[18]\tvalid_0's multi_logloss: 0.226101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 0.226114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.226388\n",
      "[21]\tvalid_0's multi_logloss: 0.22674\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.226901\n",
      "[23]\tvalid_0's multi_logloss: 0.226951\n",
      "[24]\tvalid_0's multi_logloss: 0.227024\n",
      "[25]\tvalid_0's multi_logloss: 0.227289\n",
      "[26]\tvalid_0's multi_logloss: 0.227556\n",
      "[27]\tvalid_0's multi_logloss: 0.228025\n",
      "[28]\tvalid_0's multi_logloss: 0.228324\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.22843\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.228801\n",
      "[31]\tvalid_0's multi_logloss: 0.229179\n",
      "[32]\tvalid_0's multi_logloss: 0.229406\n",
      "[33]\tvalid_0's multi_logloss: 0.229493\n",
      "[34]\tvalid_0's multi_logloss: 0.229747\n",
      "[35]\tvalid_0's multi_logloss: 0.230114\n",
      "[36]\tvalid_0's multi_logloss: 0.230326\n",
      "[37]\tvalid_0's multi_logloss: 0.230737\n",
      "[38]\tvalid_0's multi_logloss: 0.23095\n",
      "[39]\tvalid_0's multi_logloss: 0.23148\n",
      "[40]\tvalid_0's multi_logloss: 0.231771\n",
      "[41]\tvalid_0's multi_logloss: 0.232131\n",
      "[42]\tvalid_0's multi_logloss: 0.232491\n",
      "[43]\tvalid_0's multi_logloss: 0.232737\n",
      "[44]\tvalid_0's multi_logloss: 0.23316\n",
      "[45]\tvalid_0's multi_logloss: 0.23351\n",
      "[46]\tvalid_0's multi_logloss: 0.233806\n",
      "[47]\tvalid_0's multi_logloss: 0.23401\n",
      "[48]\tvalid_0's multi_logloss: 0.234209\n",
      "[49]\tvalid_0's multi_logloss: 0.234572\n",
      "[50]\tvalid_0's multi_logloss: 0.234631\n",
      "[51]\tvalid_0's multi_logloss: 0.23504\n",
      "[52]\tvalid_0's multi_logloss: 0.235349\n",
      "[53]\tvalid_0's multi_logloss: 0.23553\n",
      "[54]\tvalid_0's multi_logloss: 0.235781\n",
      "[55]\tvalid_0's multi_logloss: 0.23606\n",
      "[56]\tvalid_0's multi_logloss: 0.236594\n",
      "[57]\tvalid_0's multi_logloss: 0.236758\n",
      "[58]\tvalid_0's multi_logloss: 0.236865\n",
      "[59]\tvalid_0's multi_logloss: 0.237096\n",
      "[60]\tvalid_0's multi_logloss: 0.237475\n",
      "[61]\tvalid_0's multi_logloss: 0.237689\n",
      "[62]\tvalid_0's multi_logloss: 0.237897\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's multi_logloss: 0.238297\n",
      "[64]\tvalid_0's multi_logloss: 0.238594\n",
      "[65]\tvalid_0's multi_logloss: 0.238917\n",
      "[66]\tvalid_0's multi_logloss: 0.239015\n",
      "[67]\tvalid_0's multi_logloss: 0.239211\n",
      "[68]\tvalid_0's multi_logloss: 0.239252\n",
      "[69]\tvalid_0's multi_logloss: 0.239519\n",
      "[70]\tvalid_0's multi_logloss: 0.239808\n",
      "[71]\tvalid_0's multi_logloss: 0.239975\n",
      "[72]\tvalid_0's multi_logloss: 0.240278\n",
      "[73]\tvalid_0's multi_logloss: 0.240696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74]\tvalid_0's multi_logloss: 0.240958\n",
      "[75]\tvalid_0's multi_logloss: 0.241211\n",
      "[76]\tvalid_0's multi_logloss: 0.241437\n",
      "[77]\tvalid_0's multi_logloss: 0.24174\n",
      "[78]\tvalid_0's multi_logloss: 0.241909\n",
      "[79]\tvalid_0's multi_logloss: 0.242171\n",
      "[80]\tvalid_0's multi_logloss: 0.242445\n",
      "[81]\tvalid_0's multi_logloss: 0.242893\n",
      "[82]\tvalid_0's multi_logloss: 0.243169\n",
      "[83]\tvalid_0's multi_logloss: 0.243308\n",
      "[84]\tvalid_0's multi_logloss: 0.243393\n",
      "[85]\tvalid_0's multi_logloss: 0.243805\n",
      "[86]\tvalid_0's multi_logloss: 0.2439\n",
      "[87]\tvalid_0's multi_logloss: 0.244187\n",
      "[88]\tvalid_0's multi_logloss: 0.244425\n",
      "[89]\tvalid_0's multi_logloss: 0.244619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's multi_logloss: 0.244737\n",
      "[91]\tvalid_0's multi_logloss: 0.24508\n",
      "[92]\tvalid_0's multi_logloss: 0.245388\n",
      "[93]\tvalid_0's multi_logloss: 0.245415\n",
      "[94]\tvalid_0's multi_logloss: 0.245609\n",
      "[95]\tvalid_0's multi_logloss: 0.245686\n",
      "[96]\tvalid_0's multi_logloss: 0.245885\n",
      "[97]\tvalid_0's multi_logloss: 0.246124\n",
      "[98]\tvalid_0's multi_logloss: 0.246243\n",
      "[99]\tvalid_0's multi_logloss: 0.246533\n",
      "[100]\tvalid_0's multi_logloss: 0.246713\n",
      "[101]\tvalid_0's multi_logloss: 0.246999\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.221573\n",
      "lgb now score is: [2.5873191651232696, 2.595481008646282, 2.507570909581667]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7240\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 129\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.293873\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.293326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.292907\n",
      "[4]\tvalid_0's multi_logloss: 0.292633\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.292816\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.292437\n",
      "[7]\tvalid_0's multi_logloss: 0.292186\n",
      "[8]\tvalid_0's multi_logloss: 0.291772\n",
      "[9]\tvalid_0's multi_logloss: 0.292078\n",
      "[10]\tvalid_0's multi_logloss: 0.29211\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.29175\n",
      "[12]\tvalid_0's multi_logloss: 0.29182\n",
      "[13]\tvalid_0's multi_logloss: 0.291843\n",
      "[14]\tvalid_0's multi_logloss: 0.291702\n",
      "[15]\tvalid_0's multi_logloss: 0.291441\n",
      "[16]\tvalid_0's multi_logloss: 0.291642\n",
      "[17]\tvalid_0's multi_logloss: 0.291824\n",
      "[18]\tvalid_0's multi_logloss: 0.291817\n",
      "[19]\tvalid_0's multi_logloss: 0.291707\n",
      "[20]\tvalid_0's multi_logloss: 0.291791\n",
      "[21]\tvalid_0's multi_logloss: 0.291713\n",
      "[22]\tvalid_0's multi_logloss: 0.291812\n",
      "[23]\tvalid_0's multi_logloss: 0.291615\n",
      "[24]\tvalid_0's multi_logloss: 0.291798\n",
      "[25]\tvalid_0's multi_logloss: 0.292036\n",
      "[26]\tvalid_0's multi_logloss: 0.291748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.291729\n",
      "[28]\tvalid_0's multi_logloss: 0.291864\n",
      "[29]\tvalid_0's multi_logloss: 0.291893\n",
      "[30]\tvalid_0's multi_logloss: 0.291791\n",
      "[31]\tvalid_0's multi_logloss: 0.291895\n",
      "[32]\tvalid_0's multi_logloss: 0.292102\n",
      "[33]\tvalid_0's multi_logloss: 0.292099\n",
      "[34]\tvalid_0's multi_logloss: 0.292091\n",
      "[35]\tvalid_0's multi_logloss: 0.29217\n",
      "[36]\tvalid_0's multi_logloss: 0.292158\n",
      "[37]\tvalid_0's multi_logloss: 0.292259\n",
      "[38]\tvalid_0's multi_logloss: 0.29217\n",
      "[39]\tvalid_0's multi_logloss: 0.292334\n",
      "[40]\tvalid_0's multi_logloss: 0.292498\n",
      "[41]\tvalid_0's multi_logloss: 0.292764\n",
      "[42]\tvalid_0's multi_logloss: 0.293315\n",
      "[43]\tvalid_0's multi_logloss: 0.293399\n",
      "[44]\tvalid_0's multi_logloss: 0.293539\n",
      "[45]\tvalid_0's multi_logloss: 0.293776\n",
      "[46]\tvalid_0's multi_logloss: 0.294112\n",
      "[47]\tvalid_0's multi_logloss: 0.29434\n",
      "[48]\tvalid_0's multi_logloss: 0.294668\n",
      "[49]\tvalid_0's multi_logloss: 0.29509\n",
      "[50]\tvalid_0's multi_logloss: 0.295558\n",
      "[51]\tvalid_0's multi_logloss: 0.295893\n",
      "[52]\tvalid_0's multi_logloss: 0.296082\n",
      "[53]\tvalid_0's multi_logloss: 0.296263\n",
      "[54]\tvalid_0's multi_logloss: 0.296021\n",
      "[55]\tvalid_0's multi_logloss: 0.295901\n",
      "[56]\tvalid_0's multi_logloss: 0.296021\n",
      "[57]\tvalid_0's multi_logloss: 0.296115\n",
      "[58]\tvalid_0's multi_logloss: 0.296471\n",
      "[59]\tvalid_0's multi_logloss: 0.296696\n",
      "[60]\tvalid_0's multi_logloss: 0.296801\n",
      "[61]\tvalid_0's multi_logloss: 0.297037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.296981\n",
      "[63]\tvalid_0's multi_logloss: 0.297033\n",
      "[64]\tvalid_0's multi_logloss: 0.296953\n",
      "[65]\tvalid_0's multi_logloss: 0.297143\n",
      "[66]\tvalid_0's multi_logloss: 0.297427\n",
      "[67]\tvalid_0's multi_logloss: 0.297537\n",
      "[68]\tvalid_0's multi_logloss: 0.297827\n",
      "[69]\tvalid_0's multi_logloss: 0.297742\n",
      "[70]\tvalid_0's multi_logloss: 0.297722\n",
      "[71]\tvalid_0's multi_logloss: 0.298101\n",
      "[72]\tvalid_0's multi_logloss: 0.29836\n",
      "[73]\tvalid_0's multi_logloss: 0.298188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's multi_logloss: 0.298303\n",
      "[75]\tvalid_0's multi_logloss: 0.298247\n",
      "[76]\tvalid_0's multi_logloss: 0.298578\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's multi_logloss: 0.298631\n",
      "[78]\tvalid_0's multi_logloss: 0.298673\n",
      "[79]\tvalid_0's multi_logloss: 0.298559\n",
      "[80]\tvalid_0's multi_logloss: 0.298919\n",
      "[81]\tvalid_0's multi_logloss: 0.299047\n",
      "[82]\tvalid_0's multi_logloss: 0.299232\n",
      "[83]\tvalid_0's multi_logloss: 0.299381\n",
      "[84]\tvalid_0's multi_logloss: 0.29954\n",
      "[85]\tvalid_0's multi_logloss: 0.299348\n",
      "[86]\tvalid_0's multi_logloss: 0.299355\n",
      "[87]\tvalid_0's multi_logloss: 0.29946\n",
      "[88]\tvalid_0's multi_logloss: 0.299647\n",
      "[89]\tvalid_0's multi_logloss: 0.299895\n",
      "[90]\tvalid_0's multi_logloss: 0.299891\n",
      "[91]\tvalid_0's multi_logloss: 0.299744\n",
      "[92]\tvalid_0's multi_logloss: 0.299578\n",
      "[93]\tvalid_0's multi_logloss: 0.299582\n",
      "[94]\tvalid_0's multi_logloss: 0.299781\n",
      "[95]\tvalid_0's multi_logloss: 0.299765\n",
      "[96]\tvalid_0's multi_logloss: 0.299966\n",
      "[97]\tvalid_0's multi_logloss: 0.300098\n",
      "[98]\tvalid_0's multi_logloss: 0.300284\n",
      "[99]\tvalid_0's multi_logloss: 0.300367\n",
      "[100]\tvalid_0's multi_logloss: 0.30053\n",
      "[101]\tvalid_0's multi_logloss: 0.300908\n",
      "[102]\tvalid_0's multi_logloss: 0.300954\n",
      "[103]\tvalid_0's multi_logloss: 0.301183\n",
      "[104]\tvalid_0's multi_logloss: 0.30124\n",
      "[105]\tvalid_0's multi_logloss: 0.301431\n",
      "[106]\tvalid_0's multi_logloss: 0.301773\n",
      "[107]\tvalid_0's multi_logloss: 0.301828\n",
      "[108]\tvalid_0's multi_logloss: 0.301762\n",
      "[109]\tvalid_0's multi_logloss: 0.302025\n",
      "[110]\tvalid_0's multi_logloss: 0.302121\n",
      "[111]\tvalid_0's multi_logloss: 0.302476\n",
      "[112]\tvalid_0's multi_logloss: 0.302552\n",
      "[113]\tvalid_0's multi_logloss: 0.30252\n",
      "[114]\tvalid_0's multi_logloss: 0.302542\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tvalid_0's multi_logloss: 0.302841\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's multi_logloss: 0.291441\n",
      "lgb now score is: [2.5873191651232696, 2.595481008646282, 2.507570909581667, 2.560493384374524]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7367\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 129\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073916\n",
      "[LightGBM] [Info] Start training from score -2.641560\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.215054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.214865\n",
      "[3]\tvalid_0's multi_logloss: 0.214796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.21447\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.214393\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.214328\n",
      "[7]\tvalid_0's multi_logloss: 0.214363\n",
      "[8]\tvalid_0's multi_logloss: 0.214434\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.21458\n",
      "[10]\tvalid_0's multi_logloss: 0.214716\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.214673\n",
      "[12]\tvalid_0's multi_logloss: 0.214774\n",
      "[13]\tvalid_0's multi_logloss: 0.214811\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.214797\n",
      "[15]\tvalid_0's multi_logloss: 0.214915\n",
      "[16]\tvalid_0's multi_logloss: 0.214948\n",
      "[17]\tvalid_0's multi_logloss: 0.215211\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.215402\n",
      "[19]\tvalid_0's multi_logloss: 0.215427\n",
      "[20]\tvalid_0's multi_logloss: 0.215439\n",
      "[21]\tvalid_0's multi_logloss: 0.215423\n",
      "[22]\tvalid_0's multi_logloss: 0.215478\n",
      "[23]\tvalid_0's multi_logloss: 0.215352\n",
      "[24]\tvalid_0's multi_logloss: 0.215514\n",
      "[25]\tvalid_0's multi_logloss: 0.215698\n",
      "[26]\tvalid_0's multi_logloss: 0.215815\n",
      "[27]\tvalid_0's multi_logloss: 0.215933\n",
      "[28]\tvalid_0's multi_logloss: 0.215946\n",
      "[29]\tvalid_0's multi_logloss: 0.215912\n",
      "[30]\tvalid_0's multi_logloss: 0.216048\n",
      "[31]\tvalid_0's multi_logloss: 0.216135\n",
      "[32]\tvalid_0's multi_logloss: 0.216223\n",
      "[33]\tvalid_0's multi_logloss: 0.216298\n",
      "[34]\tvalid_0's multi_logloss: 0.216525\n",
      "[35]\tvalid_0's multi_logloss: 0.216667\n",
      "[36]\tvalid_0's multi_logloss: 0.216664\n",
      "[37]\tvalid_0's multi_logloss: 0.216944\n",
      "[38]\tvalid_0's multi_logloss: 0.21703\n",
      "[39]\tvalid_0's multi_logloss: 0.217225\n",
      "[40]\tvalid_0's multi_logloss: 0.217411\n",
      "[41]\tvalid_0's multi_logloss: 0.217466\n",
      "[42]\tvalid_0's multi_logloss: 0.217608\n",
      "[43]\tvalid_0's multi_logloss: 0.217737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44]\tvalid_0's multi_logloss: 0.217907\n",
      "[45]\tvalid_0's multi_logloss: 0.218017\n",
      "[46]\tvalid_0's multi_logloss: 0.218144\n",
      "[47]\tvalid_0's multi_logloss: 0.218513\n",
      "[48]\tvalid_0's multi_logloss: 0.218683\n",
      "[49]\tvalid_0's multi_logloss: 0.218913\n",
      "[50]\tvalid_0's multi_logloss: 0.219313\n",
      "[51]\tvalid_0's multi_logloss: 0.219464\n",
      "[52]\tvalid_0's multi_logloss: 0.219578\n",
      "[53]\tvalid_0's multi_logloss: 0.219496\n",
      "[54]\tvalid_0's multi_logloss: 0.219683\n",
      "[55]\tvalid_0's multi_logloss: 0.219667\n",
      "[56]\tvalid_0's multi_logloss: 0.219676\n",
      "[57]\tvalid_0's multi_logloss: 0.219743\n",
      "[58]\tvalid_0's multi_logloss: 0.219943\n",
      "[59]\tvalid_0's multi_logloss: 0.219924\n",
      "[60]\tvalid_0's multi_logloss: 0.220239\n",
      "[61]\tvalid_0's multi_logloss: 0.220424\n",
      "[62]\tvalid_0's multi_logloss: 0.220389\n",
      "[63]\tvalid_0's multi_logloss: 0.220474\n",
      "[64]\tvalid_0's multi_logloss: 0.220483\n",
      "[65]\tvalid_0's multi_logloss: 0.220838\n",
      "[66]\tvalid_0's multi_logloss: 0.220847\n",
      "[67]\tvalid_0's multi_logloss: 0.221146\n",
      "[68]\tvalid_0's multi_logloss: 0.221075\n",
      "[69]\tvalid_0's multi_logloss: 0.221283\n",
      "[70]\tvalid_0's multi_logloss: 0.221793\n",
      "[71]\tvalid_0's multi_logloss: 0.221633\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.22166\n",
      "[73]\tvalid_0's multi_logloss: 0.22182\n",
      "[74]\tvalid_0's multi_logloss: 0.222169\n",
      "[75]\tvalid_0's multi_logloss: 0.222378\n",
      "[76]\tvalid_0's multi_logloss: 0.222348\n",
      "[77]\tvalid_0's multi_logloss: 0.222326\n",
      "[78]\tvalid_0's multi_logloss: 0.222457\n",
      "[79]\tvalid_0's multi_logloss: 0.222371\n",
      "[80]\tvalid_0's multi_logloss: 0.222536\n",
      "[81]\tvalid_0's multi_logloss: 0.222656\n",
      "[82]\tvalid_0's multi_logloss: 0.222575\n",
      "[83]\tvalid_0's multi_logloss: 0.222799\n",
      "[84]\tvalid_0's multi_logloss: 0.222808\n",
      "[85]\tvalid_0's multi_logloss: 0.222906\n",
      "[86]\tvalid_0's multi_logloss: 0.222897\n",
      "[87]\tvalid_0's multi_logloss: 0.22282\n",
      "[88]\tvalid_0's multi_logloss: 0.222891\n",
      "[89]\tvalid_0's multi_logloss: 0.222868\n",
      "[90]\tvalid_0's multi_logloss: 0.222897\n",
      "[91]\tvalid_0's multi_logloss: 0.223256\n",
      "[92]\tvalid_0's multi_logloss: 0.223429\n",
      "[93]\tvalid_0's multi_logloss: 0.223709\n",
      "[94]\tvalid_0's multi_logloss: 0.22385\n",
      "[95]\tvalid_0's multi_logloss: 0.223762\n",
      "[96]\tvalid_0's multi_logloss: 0.223861\n",
      "[97]\tvalid_0's multi_logloss: 0.223951\n",
      "[98]\tvalid_0's multi_logloss: 0.223991\n",
      "[99]\tvalid_0's multi_logloss: 0.224047\n",
      "[100]\tvalid_0's multi_logloss: 0.22416\n",
      "[101]\tvalid_0's multi_logloss: 0.224103\n",
      "[102]\tvalid_0's multi_logloss: 0.22428\n",
      "[103]\tvalid_0's multi_logloss: 0.224347\n",
      "[104]\tvalid_0's multi_logloss: 0.224609\n",
      "[105]\tvalid_0's multi_logloss: 0.22497\n",
      "[106]\tvalid_0's multi_logloss: 0.224912\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's multi_logloss: 0.214328\n",
      "lgb now score is: [2.5873191651232696, 2.595481008646282, 2.507570909581667, 2.560493384374524, 2.522709368328998]\n",
      "lgb_score_list: [2.5873191651232696, 2.595481008646282, 2.507570909581667, 2.560493384374524, 2.522709368328998]\n",
      "lgb_score_mean: 2.5547147672109483\n",
      "[0]\ttrain-mlogloss:0.67094\teval-mlogloss:0.67221\n",
      "[1]\ttrain-mlogloss:0.65008\teval-mlogloss:0.65256\n",
      "[2]\ttrain-mlogloss:0.63004\teval-mlogloss:0.63384\n",
      "[3]\ttrain-mlogloss:0.61126\teval-mlogloss:0.61636\n",
      "[4]\ttrain-mlogloss:0.59361\teval-mlogloss:0.59984\n",
      "[5]\ttrain-mlogloss:0.57672\teval-mlogloss:0.58427\n",
      "[6]\ttrain-mlogloss:0.56085\teval-mlogloss:0.56931\n",
      "[7]\ttrain-mlogloss:0.54581\teval-mlogloss:0.55512\n",
      "[8]\ttrain-mlogloss:0.53150\teval-mlogloss:0.54195\n",
      "[9]\ttrain-mlogloss:0.51790\teval-mlogloss:0.52933\n",
      "[10]\ttrain-mlogloss:0.50496\teval-mlogloss:0.51734\n",
      "[11]\ttrain-mlogloss:0.49278\teval-mlogloss:0.50633\n",
      "[12]\ttrain-mlogloss:0.48096\teval-mlogloss:0.49541\n",
      "[13]\ttrain-mlogloss:0.46968\teval-mlogloss:0.48531\n",
      "[14]\ttrain-mlogloss:0.45904\teval-mlogloss:0.47552\n",
      "[15]\ttrain-mlogloss:0.44879\teval-mlogloss:0.46621\n",
      "[16]\ttrain-mlogloss:0.43911\teval-mlogloss:0.45742\n",
      "[17]\ttrain-mlogloss:0.42993\teval-mlogloss:0.44913\n",
      "[18]\ttrain-mlogloss:0.42102\teval-mlogloss:0.44132\n",
      "[19]\ttrain-mlogloss:0.41253\teval-mlogloss:0.43369\n",
      "[20]\ttrain-mlogloss:0.40437\teval-mlogloss:0.42654\n",
      "[21]\ttrain-mlogloss:0.39621\teval-mlogloss:0.41920\n",
      "[22]\ttrain-mlogloss:0.38871\teval-mlogloss:0.41267\n",
      "[23]\ttrain-mlogloss:0.38141\teval-mlogloss:0.40628\n",
      "[24]\ttrain-mlogloss:0.37456\teval-mlogloss:0.40037\n",
      "[25]\ttrain-mlogloss:0.36788\teval-mlogloss:0.39463\n",
      "[26]\ttrain-mlogloss:0.36161\teval-mlogloss:0.38917\n",
      "[27]\ttrain-mlogloss:0.35566\teval-mlogloss:0.38426\n",
      "[28]\ttrain-mlogloss:0.34976\teval-mlogloss:0.37924\n",
      "[29]\ttrain-mlogloss:0.34423\teval-mlogloss:0.37460\n",
      "[30]\ttrain-mlogloss:0.33887\teval-mlogloss:0.37018\n",
      "[31]\ttrain-mlogloss:0.33386\teval-mlogloss:0.36611\n",
      "[32]\ttrain-mlogloss:0.32905\teval-mlogloss:0.36227\n",
      "[33]\ttrain-mlogloss:0.32440\teval-mlogloss:0.35843\n",
      "[34]\ttrain-mlogloss:0.31984\teval-mlogloss:0.35454\n",
      "[35]\ttrain-mlogloss:0.31536\teval-mlogloss:0.35089\n",
      "[36]\ttrain-mlogloss:0.31109\teval-mlogloss:0.34736\n",
      "[37]\ttrain-mlogloss:0.30696\teval-mlogloss:0.34416\n",
      "[38]\ttrain-mlogloss:0.30306\teval-mlogloss:0.34129\n",
      "[39]\ttrain-mlogloss:0.29941\teval-mlogloss:0.33840\n",
      "[40]\ttrain-mlogloss:0.29594\teval-mlogloss:0.33572\n",
      "[41]\ttrain-mlogloss:0.29250\teval-mlogloss:0.33314\n",
      "[42]\ttrain-mlogloss:0.28918\teval-mlogloss:0.33077\n",
      "[43]\ttrain-mlogloss:0.28602\teval-mlogloss:0.32842\n",
      "[44]\ttrain-mlogloss:0.28286\teval-mlogloss:0.32622\n",
      "[45]\ttrain-mlogloss:0.27975\teval-mlogloss:0.32427\n",
      "[46]\ttrain-mlogloss:0.27696\teval-mlogloss:0.32217\n",
      "[47]\ttrain-mlogloss:0.27411\teval-mlogloss:0.32040\n",
      "[48]\ttrain-mlogloss:0.27143\teval-mlogloss:0.31858\n",
      "[49]\ttrain-mlogloss:0.26873\teval-mlogloss:0.31670\n",
      "[50]\ttrain-mlogloss:0.26630\teval-mlogloss:0.31504\n",
      "[51]\ttrain-mlogloss:0.26377\teval-mlogloss:0.31347\n",
      "[52]\ttrain-mlogloss:0.26133\teval-mlogloss:0.31219\n",
      "[53]\ttrain-mlogloss:0.25914\teval-mlogloss:0.31081\n",
      "[54]\ttrain-mlogloss:0.25694\teval-mlogloss:0.30945\n",
      "[55]\ttrain-mlogloss:0.25478\teval-mlogloss:0.30819\n",
      "[56]\ttrain-mlogloss:0.25290\teval-mlogloss:0.30708\n",
      "[57]\ttrain-mlogloss:0.25094\teval-mlogloss:0.30589\n",
      "[58]\ttrain-mlogloss:0.24894\teval-mlogloss:0.30466\n",
      "[59]\ttrain-mlogloss:0.24703\teval-mlogloss:0.30377\n",
      "[60]\ttrain-mlogloss:0.24541\teval-mlogloss:0.30284\n",
      "[61]\ttrain-mlogloss:0.24369\teval-mlogloss:0.30202\n",
      "[62]\ttrain-mlogloss:0.24197\teval-mlogloss:0.30092\n",
      "[63]\ttrain-mlogloss:0.24018\teval-mlogloss:0.30026\n",
      "[64]\ttrain-mlogloss:0.23851\teval-mlogloss:0.29956\n",
      "[65]\ttrain-mlogloss:0.23709\teval-mlogloss:0.29889\n",
      "[66]\ttrain-mlogloss:0.23579\teval-mlogloss:0.29837\n",
      "[67]\ttrain-mlogloss:0.23435\teval-mlogloss:0.29779\n",
      "[68]\ttrain-mlogloss:0.23293\teval-mlogloss:0.29727\n",
      "[69]\ttrain-mlogloss:0.23162\teval-mlogloss:0.29674\n",
      "[70]\ttrain-mlogloss:0.23041\teval-mlogloss:0.29618\n",
      "[71]\ttrain-mlogloss:0.22921\teval-mlogloss:0.29561\n",
      "[72]\ttrain-mlogloss:0.22785\teval-mlogloss:0.29508\n",
      "[73]\ttrain-mlogloss:0.22662\teval-mlogloss:0.29462\n",
      "[74]\ttrain-mlogloss:0.22533\teval-mlogloss:0.29411\n",
      "[75]\ttrain-mlogloss:0.22429\teval-mlogloss:0.29355\n",
      "[76]\ttrain-mlogloss:0.22324\teval-mlogloss:0.29345\n",
      "[77]\ttrain-mlogloss:0.22221\teval-mlogloss:0.29327\n",
      "[78]\ttrain-mlogloss:0.22108\teval-mlogloss:0.29310\n",
      "[79]\ttrain-mlogloss:0.21984\teval-mlogloss:0.29287\n",
      "[80]\ttrain-mlogloss:0.21895\teval-mlogloss:0.29267\n",
      "[81]\ttrain-mlogloss:0.21824\teval-mlogloss:0.29253\n",
      "[82]\ttrain-mlogloss:0.21730\teval-mlogloss:0.29230\n",
      "[83]\ttrain-mlogloss:0.21646\teval-mlogloss:0.29214\n",
      "[84]\ttrain-mlogloss:0.21557\teval-mlogloss:0.29211\n",
      "[85]\ttrain-mlogloss:0.21460\teval-mlogloss:0.29185\n",
      "[86]\ttrain-mlogloss:0.21370\teval-mlogloss:0.29192\n",
      "[87]\ttrain-mlogloss:0.21289\teval-mlogloss:0.29161\n",
      "[88]\ttrain-mlogloss:0.21200\teval-mlogloss:0.29154\n",
      "[89]\ttrain-mlogloss:0.21131\teval-mlogloss:0.29161\n",
      "[90]\ttrain-mlogloss:0.21056\teval-mlogloss:0.29147\n",
      "[91]\ttrain-mlogloss:0.20990\teval-mlogloss:0.29148\n",
      "[92]\ttrain-mlogloss:0.20905\teval-mlogloss:0.29150\n",
      "[93]\ttrain-mlogloss:0.20846\teval-mlogloss:0.29158\n",
      "[94]\ttrain-mlogloss:0.20773\teval-mlogloss:0.29153\n",
      "[95]\ttrain-mlogloss:0.20697\teval-mlogloss:0.29173\n",
      "[96]\ttrain-mlogloss:0.20629\teval-mlogloss:0.29183\n",
      "[97]\ttrain-mlogloss:0.20561\teval-mlogloss:0.29194\n",
      "[98]\ttrain-mlogloss:0.20490\teval-mlogloss:0.29203\n",
      "[99]\ttrain-mlogloss:0.20424\teval-mlogloss:0.29207\n",
      "[100]\ttrain-mlogloss:0.20354\teval-mlogloss:0.29187\n",
      "[101]\ttrain-mlogloss:0.20294\teval-mlogloss:0.29195\n",
      "[102]\ttrain-mlogloss:0.20223\teval-mlogloss:0.29188\n",
      "[103]\ttrain-mlogloss:0.20154\teval-mlogloss:0.29182\n",
      "[104]\ttrain-mlogloss:0.20082\teval-mlogloss:0.29159\n",
      "[105]\ttrain-mlogloss:0.20015\teval-mlogloss:0.29162\n",
      "[106]\ttrain-mlogloss:0.19951\teval-mlogloss:0.29165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107]\ttrain-mlogloss:0.19899\teval-mlogloss:0.29172\n",
      "[108]\ttrain-mlogloss:0.19830\teval-mlogloss:0.29185\n",
      "[109]\ttrain-mlogloss:0.19783\teval-mlogloss:0.29198\n",
      "[110]\ttrain-mlogloss:0.19715\teval-mlogloss:0.29224\n",
      "[111]\ttrain-mlogloss:0.19649\teval-mlogloss:0.29236\n",
      "[112]\ttrain-mlogloss:0.19575\teval-mlogloss:0.29224\n",
      "[113]\ttrain-mlogloss:0.19517\teval-mlogloss:0.29216\n",
      "[114]\ttrain-mlogloss:0.19449\teval-mlogloss:0.29238\n",
      "[115]\ttrain-mlogloss:0.19395\teval-mlogloss:0.29254\n",
      "[116]\ttrain-mlogloss:0.19331\teval-mlogloss:0.29264\n",
      "[117]\ttrain-mlogloss:0.19265\teval-mlogloss:0.29276\n",
      "[118]\ttrain-mlogloss:0.19214\teval-mlogloss:0.29283\n",
      "[119]\ttrain-mlogloss:0.19172\teval-mlogloss:0.29302\n",
      "[120]\ttrain-mlogloss:0.19115\teval-mlogloss:0.29296\n",
      "[121]\ttrain-mlogloss:0.19056\teval-mlogloss:0.29282\n",
      "[122]\ttrain-mlogloss:0.19000\teval-mlogloss:0.29294\n",
      "[123]\ttrain-mlogloss:0.18929\teval-mlogloss:0.29311\n",
      "[124]\ttrain-mlogloss:0.18879\teval-mlogloss:0.29308\n",
      "[125]\ttrain-mlogloss:0.18836\teval-mlogloss:0.29323\n",
      "[126]\ttrain-mlogloss:0.18797\teval-mlogloss:0.29335\n",
      "[127]\ttrain-mlogloss:0.18741\teval-mlogloss:0.29362\n",
      "[128]\ttrain-mlogloss:0.18694\teval-mlogloss:0.29375\n",
      "[129]\ttrain-mlogloss:0.18655\teval-mlogloss:0.29402\n",
      "[130]\ttrain-mlogloss:0.18612\teval-mlogloss:0.29411\n",
      "[131]\ttrain-mlogloss:0.18552\teval-mlogloss:0.29428\n",
      "[132]\ttrain-mlogloss:0.18495\teval-mlogloss:0.29421\n",
      "[133]\ttrain-mlogloss:0.18457\teval-mlogloss:0.29437\n",
      "[134]\ttrain-mlogloss:0.18411\teval-mlogloss:0.29455\n",
      "[135]\ttrain-mlogloss:0.18366\teval-mlogloss:0.29500\n",
      "[136]\ttrain-mlogloss:0.18315\teval-mlogloss:0.29516\n",
      "[137]\ttrain-mlogloss:0.18266\teval-mlogloss:0.29522\n",
      "[138]\ttrain-mlogloss:0.18224\teval-mlogloss:0.29536\n",
      "[139]\ttrain-mlogloss:0.18185\teval-mlogloss:0.29541\n",
      "[140]\ttrain-mlogloss:0.18129\teval-mlogloss:0.29533\n",
      "[141]\ttrain-mlogloss:0.18077\teval-mlogloss:0.29548\n",
      "[142]\ttrain-mlogloss:0.18043\teval-mlogloss:0.29536\n",
      "[143]\ttrain-mlogloss:0.18004\teval-mlogloss:0.29518\n",
      "[144]\ttrain-mlogloss:0.17959\teval-mlogloss:0.29515\n",
      "[145]\ttrain-mlogloss:0.17918\teval-mlogloss:0.29527\n",
      "[146]\ttrain-mlogloss:0.17883\teval-mlogloss:0.29537\n",
      "[147]\ttrain-mlogloss:0.17842\teval-mlogloss:0.29559\n",
      "[148]\ttrain-mlogloss:0.17797\teval-mlogloss:0.29571\n",
      "[149]\ttrain-mlogloss:0.17746\teval-mlogloss:0.29577\n",
      "[150]\ttrain-mlogloss:0.17706\teval-mlogloss:0.29592\n",
      "[151]\ttrain-mlogloss:0.17654\teval-mlogloss:0.29607\n",
      "[152]\ttrain-mlogloss:0.17598\teval-mlogloss:0.29609\n",
      "[153]\ttrain-mlogloss:0.17542\teval-mlogloss:0.29615\n",
      "[154]\ttrain-mlogloss:0.17478\teval-mlogloss:0.29635\n",
      "[155]\ttrain-mlogloss:0.17441\teval-mlogloss:0.29642\n",
      "[156]\ttrain-mlogloss:0.17404\teval-mlogloss:0.29668\n",
      "[157]\ttrain-mlogloss:0.17357\teval-mlogloss:0.29665\n",
      "[158]\ttrain-mlogloss:0.17315\teval-mlogloss:0.29659\n",
      "[159]\ttrain-mlogloss:0.17282\teval-mlogloss:0.29666\n",
      "[160]\ttrain-mlogloss:0.17236\teval-mlogloss:0.29675\n",
      "[161]\ttrain-mlogloss:0.17197\teval-mlogloss:0.29681\n",
      "[162]\ttrain-mlogloss:0.17152\teval-mlogloss:0.29700\n",
      "[163]\ttrain-mlogloss:0.17128\teval-mlogloss:0.29697\n",
      "[164]\ttrain-mlogloss:0.17098\teval-mlogloss:0.29702\n",
      "[165]\ttrain-mlogloss:0.17056\teval-mlogloss:0.29714\n",
      "[166]\ttrain-mlogloss:0.17028\teval-mlogloss:0.29709\n",
      "[167]\ttrain-mlogloss:0.16990\teval-mlogloss:0.29736\n",
      "[168]\ttrain-mlogloss:0.16952\teval-mlogloss:0.29742\n",
      "[169]\ttrain-mlogloss:0.16917\teval-mlogloss:0.29759\n",
      "[170]\ttrain-mlogloss:0.16878\teval-mlogloss:0.29763\n",
      "[171]\ttrain-mlogloss:0.16848\teval-mlogloss:0.29768\n",
      "[172]\ttrain-mlogloss:0.16814\teval-mlogloss:0.29765\n",
      "[173]\ttrain-mlogloss:0.16781\teval-mlogloss:0.29773\n",
      "[174]\ttrain-mlogloss:0.16735\teval-mlogloss:0.29777\n",
      "[175]\ttrain-mlogloss:0.16708\teval-mlogloss:0.29793\n",
      "[176]\ttrain-mlogloss:0.16668\teval-mlogloss:0.29799\n",
      "[177]\ttrain-mlogloss:0.16631\teval-mlogloss:0.29792\n",
      "[178]\ttrain-mlogloss:0.16603\teval-mlogloss:0.29804\n",
      "[179]\ttrain-mlogloss:0.16576\teval-mlogloss:0.29783\n",
      "[180]\ttrain-mlogloss:0.16539\teval-mlogloss:0.29771\n",
      "[181]\ttrain-mlogloss:0.16497\teval-mlogloss:0.29799\n",
      "[182]\ttrain-mlogloss:0.16462\teval-mlogloss:0.29821\n",
      "[183]\ttrain-mlogloss:0.16433\teval-mlogloss:0.29831\n",
      "[184]\ttrain-mlogloss:0.16389\teval-mlogloss:0.29861\n",
      "[185]\ttrain-mlogloss:0.16362\teval-mlogloss:0.29871\n",
      "[186]\ttrain-mlogloss:0.16328\teval-mlogloss:0.29891\n",
      "[187]\ttrain-mlogloss:0.16304\teval-mlogloss:0.29891\n",
      "[188]\ttrain-mlogloss:0.16261\teval-mlogloss:0.29931\n",
      "[189]\ttrain-mlogloss:0.16228\teval-mlogloss:0.29953\n",
      "xgb now score is: [2.2251069462671875]\n",
      "[0]\ttrain-mlogloss:0.67144\teval-mlogloss:0.67104\n",
      "[1]\ttrain-mlogloss:0.65121\teval-mlogloss:0.65014\n",
      "[2]\ttrain-mlogloss:0.63200\teval-mlogloss:0.63083\n",
      "[3]\ttrain-mlogloss:0.61386\teval-mlogloss:0.61226\n",
      "[4]\ttrain-mlogloss:0.59645\teval-mlogloss:0.59466\n",
      "[5]\ttrain-mlogloss:0.58023\teval-mlogloss:0.57805\n",
      "[6]\ttrain-mlogloss:0.56482\teval-mlogloss:0.56244\n",
      "[7]\ttrain-mlogloss:0.55020\teval-mlogloss:0.54758\n",
      "[8]\ttrain-mlogloss:0.53620\teval-mlogloss:0.53335\n",
      "[9]\ttrain-mlogloss:0.52282\teval-mlogloss:0.51975\n",
      "[10]\ttrain-mlogloss:0.51035\teval-mlogloss:0.50697\n",
      "[11]\ttrain-mlogloss:0.49834\teval-mlogloss:0.49469\n",
      "[12]\ttrain-mlogloss:0.48693\teval-mlogloss:0.48315\n",
      "[13]\ttrain-mlogloss:0.47614\teval-mlogloss:0.47220\n",
      "[14]\ttrain-mlogloss:0.46560\teval-mlogloss:0.46142\n",
      "[15]\ttrain-mlogloss:0.45579\teval-mlogloss:0.45142\n",
      "[16]\ttrain-mlogloss:0.44638\teval-mlogloss:0.44162\n",
      "[17]\ttrain-mlogloss:0.43729\teval-mlogloss:0.43255\n",
      "[18]\ttrain-mlogloss:0.42861\teval-mlogloss:0.42374\n",
      "[19]\ttrain-mlogloss:0.42036\teval-mlogloss:0.41520\n",
      "[20]\ttrain-mlogloss:0.41259\teval-mlogloss:0.40714\n",
      "[21]\ttrain-mlogloss:0.40489\teval-mlogloss:0.39907\n",
      "[22]\ttrain-mlogloss:0.39776\teval-mlogloss:0.39169\n",
      "[23]\ttrain-mlogloss:0.39088\teval-mlogloss:0.38451\n",
      "[24]\ttrain-mlogloss:0.38406\teval-mlogloss:0.37773\n",
      "[25]\ttrain-mlogloss:0.37792\teval-mlogloss:0.37136\n",
      "[26]\ttrain-mlogloss:0.37192\teval-mlogloss:0.36523\n",
      "[27]\ttrain-mlogloss:0.36618\teval-mlogloss:0.35924\n",
      "[28]\ttrain-mlogloss:0.36077\teval-mlogloss:0.35374\n",
      "[29]\ttrain-mlogloss:0.35546\teval-mlogloss:0.34837\n",
      "[30]\ttrain-mlogloss:0.35039\teval-mlogloss:0.34316\n",
      "[31]\ttrain-mlogloss:0.34551\teval-mlogloss:0.33818\n",
      "[32]\ttrain-mlogloss:0.34084\teval-mlogloss:0.33336\n",
      "[33]\ttrain-mlogloss:0.33632\teval-mlogloss:0.32893\n",
      "[34]\ttrain-mlogloss:0.33201\teval-mlogloss:0.32472\n",
      "[35]\ttrain-mlogloss:0.32793\teval-mlogloss:0.32042\n",
      "[36]\ttrain-mlogloss:0.32384\teval-mlogloss:0.31634\n",
      "[37]\ttrain-mlogloss:0.32004\teval-mlogloss:0.31250\n",
      "[38]\ttrain-mlogloss:0.31639\teval-mlogloss:0.30878\n",
      "[39]\ttrain-mlogloss:0.31279\teval-mlogloss:0.30534\n",
      "[40]\ttrain-mlogloss:0.30939\teval-mlogloss:0.30202\n",
      "[41]\ttrain-mlogloss:0.30599\teval-mlogloss:0.29853\n",
      "[42]\ttrain-mlogloss:0.30298\teval-mlogloss:0.29552\n",
      "[43]\ttrain-mlogloss:0.29976\teval-mlogloss:0.29247\n",
      "[44]\ttrain-mlogloss:0.29686\teval-mlogloss:0.28952\n",
      "[45]\ttrain-mlogloss:0.29390\teval-mlogloss:0.28684\n",
      "[46]\ttrain-mlogloss:0.29136\teval-mlogloss:0.28431\n",
      "[47]\ttrain-mlogloss:0.28878\teval-mlogloss:0.28171\n",
      "[48]\ttrain-mlogloss:0.28622\teval-mlogloss:0.27931\n",
      "[49]\ttrain-mlogloss:0.28370\teval-mlogloss:0.27686\n",
      "[50]\ttrain-mlogloss:0.28138\teval-mlogloss:0.27461\n",
      "[51]\ttrain-mlogloss:0.27907\teval-mlogloss:0.27218\n",
      "[52]\ttrain-mlogloss:0.27689\teval-mlogloss:0.26987\n",
      "[53]\ttrain-mlogloss:0.27463\teval-mlogloss:0.26790\n",
      "[54]\ttrain-mlogloss:0.27249\teval-mlogloss:0.26575\n",
      "[55]\ttrain-mlogloss:0.27045\teval-mlogloss:0.26380\n",
      "[56]\ttrain-mlogloss:0.26864\teval-mlogloss:0.26193\n",
      "[57]\ttrain-mlogloss:0.26694\teval-mlogloss:0.26031\n",
      "[58]\ttrain-mlogloss:0.26500\teval-mlogloss:0.25872\n",
      "[59]\ttrain-mlogloss:0.26337\teval-mlogloss:0.25701\n",
      "[60]\ttrain-mlogloss:0.26151\teval-mlogloss:0.25533\n",
      "[61]\ttrain-mlogloss:0.25981\teval-mlogloss:0.25375\n",
      "[62]\ttrain-mlogloss:0.25839\teval-mlogloss:0.25249\n",
      "[63]\ttrain-mlogloss:0.25674\teval-mlogloss:0.25094\n",
      "[64]\ttrain-mlogloss:0.25514\teval-mlogloss:0.24965\n",
      "[65]\ttrain-mlogloss:0.25376\teval-mlogloss:0.24864\n",
      "[66]\ttrain-mlogloss:0.25254\teval-mlogloss:0.24741\n",
      "[67]\ttrain-mlogloss:0.25124\teval-mlogloss:0.24618\n",
      "[68]\ttrain-mlogloss:0.24994\teval-mlogloss:0.24483\n",
      "[69]\ttrain-mlogloss:0.24869\teval-mlogloss:0.24351\n",
      "[70]\ttrain-mlogloss:0.24761\teval-mlogloss:0.24255\n",
      "[71]\ttrain-mlogloss:0.24640\teval-mlogloss:0.24154\n",
      "[72]\ttrain-mlogloss:0.24531\teval-mlogloss:0.24075\n",
      "[73]\ttrain-mlogloss:0.24419\teval-mlogloss:0.23979\n",
      "[74]\ttrain-mlogloss:0.24323\teval-mlogloss:0.23904\n",
      "[75]\ttrain-mlogloss:0.24234\teval-mlogloss:0.23809\n",
      "[76]\ttrain-mlogloss:0.24123\teval-mlogloss:0.23740\n",
      "[77]\ttrain-mlogloss:0.23998\teval-mlogloss:0.23666\n",
      "[78]\ttrain-mlogloss:0.23900\teval-mlogloss:0.23588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79]\ttrain-mlogloss:0.23827\teval-mlogloss:0.23526\n",
      "[80]\ttrain-mlogloss:0.23732\teval-mlogloss:0.23458\n",
      "[81]\ttrain-mlogloss:0.23651\teval-mlogloss:0.23403\n",
      "[82]\ttrain-mlogloss:0.23540\teval-mlogloss:0.23340\n",
      "[83]\ttrain-mlogloss:0.23444\teval-mlogloss:0.23264\n",
      "[84]\ttrain-mlogloss:0.23348\teval-mlogloss:0.23199\n",
      "[85]\ttrain-mlogloss:0.23249\teval-mlogloss:0.23128\n",
      "[86]\ttrain-mlogloss:0.23179\teval-mlogloss:0.23077\n",
      "[87]\ttrain-mlogloss:0.23095\teval-mlogloss:0.23048\n",
      "[88]\ttrain-mlogloss:0.23023\teval-mlogloss:0.22990\n",
      "[89]\ttrain-mlogloss:0.22937\teval-mlogloss:0.22952\n",
      "[90]\ttrain-mlogloss:0.22853\teval-mlogloss:0.22913\n",
      "[91]\ttrain-mlogloss:0.22784\teval-mlogloss:0.22859\n",
      "[92]\ttrain-mlogloss:0.22709\teval-mlogloss:0.22845\n",
      "[93]\ttrain-mlogloss:0.22633\teval-mlogloss:0.22792\n",
      "[94]\ttrain-mlogloss:0.22566\teval-mlogloss:0.22747\n",
      "[95]\ttrain-mlogloss:0.22481\teval-mlogloss:0.22719\n",
      "[96]\ttrain-mlogloss:0.22415\teval-mlogloss:0.22671\n",
      "[97]\ttrain-mlogloss:0.22349\teval-mlogloss:0.22637\n",
      "[98]\ttrain-mlogloss:0.22260\teval-mlogloss:0.22603\n",
      "[99]\ttrain-mlogloss:0.22208\teval-mlogloss:0.22571\n",
      "[100]\ttrain-mlogloss:0.22133\teval-mlogloss:0.22560\n",
      "[101]\ttrain-mlogloss:0.22072\teval-mlogloss:0.22520\n",
      "[102]\ttrain-mlogloss:0.21983\teval-mlogloss:0.22483\n",
      "[103]\ttrain-mlogloss:0.21920\teval-mlogloss:0.22439\n",
      "[104]\ttrain-mlogloss:0.21854\teval-mlogloss:0.22418\n",
      "[105]\ttrain-mlogloss:0.21796\teval-mlogloss:0.22397\n",
      "[106]\ttrain-mlogloss:0.21719\teval-mlogloss:0.22381\n",
      "[107]\ttrain-mlogloss:0.21655\teval-mlogloss:0.22341\n",
      "[108]\ttrain-mlogloss:0.21579\teval-mlogloss:0.22311\n",
      "[109]\ttrain-mlogloss:0.21522\teval-mlogloss:0.22308\n",
      "[110]\ttrain-mlogloss:0.21459\teval-mlogloss:0.22301\n",
      "[111]\ttrain-mlogloss:0.21391\teval-mlogloss:0.22292\n",
      "[112]\ttrain-mlogloss:0.21330\teval-mlogloss:0.22291\n",
      "[113]\ttrain-mlogloss:0.21263\teval-mlogloss:0.22277\n",
      "[114]\ttrain-mlogloss:0.21199\teval-mlogloss:0.22248\n",
      "[115]\ttrain-mlogloss:0.21138\teval-mlogloss:0.22233\n",
      "[116]\ttrain-mlogloss:0.21065\teval-mlogloss:0.22205\n",
      "[117]\ttrain-mlogloss:0.21003\teval-mlogloss:0.22188\n",
      "[118]\ttrain-mlogloss:0.20931\teval-mlogloss:0.22185\n",
      "[119]\ttrain-mlogloss:0.20861\teval-mlogloss:0.22178\n",
      "[120]\ttrain-mlogloss:0.20823\teval-mlogloss:0.22185\n",
      "[121]\ttrain-mlogloss:0.20765\teval-mlogloss:0.22161\n",
      "[122]\ttrain-mlogloss:0.20710\teval-mlogloss:0.22166\n",
      "[123]\ttrain-mlogloss:0.20648\teval-mlogloss:0.22171\n",
      "[124]\ttrain-mlogloss:0.20595\teval-mlogloss:0.22148\n",
      "[125]\ttrain-mlogloss:0.20529\teval-mlogloss:0.22129\n",
      "[126]\ttrain-mlogloss:0.20459\teval-mlogloss:0.22125\n",
      "[127]\ttrain-mlogloss:0.20389\teval-mlogloss:0.22123\n",
      "[128]\ttrain-mlogloss:0.20332\teval-mlogloss:0.22104\n",
      "[129]\ttrain-mlogloss:0.20271\teval-mlogloss:0.22092\n",
      "[130]\ttrain-mlogloss:0.20220\teval-mlogloss:0.22084\n",
      "[131]\ttrain-mlogloss:0.20169\teval-mlogloss:0.22045\n",
      "[132]\ttrain-mlogloss:0.20113\teval-mlogloss:0.22036\n",
      "[133]\ttrain-mlogloss:0.20051\teval-mlogloss:0.22012\n",
      "[134]\ttrain-mlogloss:0.20005\teval-mlogloss:0.22021\n",
      "[135]\ttrain-mlogloss:0.19957\teval-mlogloss:0.22017\n",
      "[136]\ttrain-mlogloss:0.19913\teval-mlogloss:0.22027\n",
      "[137]\ttrain-mlogloss:0.19867\teval-mlogloss:0.22034\n",
      "[138]\ttrain-mlogloss:0.19820\teval-mlogloss:0.22042\n",
      "[139]\ttrain-mlogloss:0.19778\teval-mlogloss:0.22046\n",
      "[140]\ttrain-mlogloss:0.19728\teval-mlogloss:0.22050\n",
      "[141]\ttrain-mlogloss:0.19675\teval-mlogloss:0.22049\n",
      "[142]\ttrain-mlogloss:0.19624\teval-mlogloss:0.22050\n",
      "[143]\ttrain-mlogloss:0.19576\teval-mlogloss:0.22053\n",
      "[144]\ttrain-mlogloss:0.19529\teval-mlogloss:0.22046\n",
      "[145]\ttrain-mlogloss:0.19482\teval-mlogloss:0.22066\n",
      "[146]\ttrain-mlogloss:0.19421\teval-mlogloss:0.22078\n",
      "[147]\ttrain-mlogloss:0.19383\teval-mlogloss:0.22071\n",
      "[148]\ttrain-mlogloss:0.19325\teval-mlogloss:0.22068\n",
      "[149]\ttrain-mlogloss:0.19278\teval-mlogloss:0.22066\n",
      "[150]\ttrain-mlogloss:0.19235\teval-mlogloss:0.22054\n",
      "[151]\ttrain-mlogloss:0.19184\teval-mlogloss:0.22032\n",
      "[152]\ttrain-mlogloss:0.19140\teval-mlogloss:0.22018\n",
      "[153]\ttrain-mlogloss:0.19113\teval-mlogloss:0.21998\n",
      "[154]\ttrain-mlogloss:0.19068\teval-mlogloss:0.21984\n",
      "[155]\ttrain-mlogloss:0.19022\teval-mlogloss:0.21976\n",
      "[156]\ttrain-mlogloss:0.18977\teval-mlogloss:0.21961\n",
      "[157]\ttrain-mlogloss:0.18938\teval-mlogloss:0.21966\n",
      "[158]\ttrain-mlogloss:0.18890\teval-mlogloss:0.21977\n",
      "[159]\ttrain-mlogloss:0.18852\teval-mlogloss:0.21971\n",
      "[160]\ttrain-mlogloss:0.18815\teval-mlogloss:0.21968\n",
      "[161]\ttrain-mlogloss:0.18769\teval-mlogloss:0.21980\n",
      "[162]\ttrain-mlogloss:0.18713\teval-mlogloss:0.21981\n",
      "[163]\ttrain-mlogloss:0.18676\teval-mlogloss:0.21969\n",
      "[164]\ttrain-mlogloss:0.18626\teval-mlogloss:0.21992\n",
      "[165]\ttrain-mlogloss:0.18577\teval-mlogloss:0.21997\n",
      "[166]\ttrain-mlogloss:0.18540\teval-mlogloss:0.22020\n",
      "[167]\ttrain-mlogloss:0.18504\teval-mlogloss:0.22013\n",
      "[168]\ttrain-mlogloss:0.18467\teval-mlogloss:0.22017\n",
      "[169]\ttrain-mlogloss:0.18426\teval-mlogloss:0.22051\n",
      "[170]\ttrain-mlogloss:0.18386\teval-mlogloss:0.22059\n",
      "[171]\ttrain-mlogloss:0.18350\teval-mlogloss:0.22062\n",
      "[172]\ttrain-mlogloss:0.18316\teval-mlogloss:0.22042\n",
      "[173]\ttrain-mlogloss:0.18275\teval-mlogloss:0.22036\n",
      "[174]\ttrain-mlogloss:0.18227\teval-mlogloss:0.22040\n",
      "[175]\ttrain-mlogloss:0.18175\teval-mlogloss:0.22035\n",
      "[176]\ttrain-mlogloss:0.18130\teval-mlogloss:0.22018\n",
      "[177]\ttrain-mlogloss:0.18090\teval-mlogloss:0.22021\n",
      "[178]\ttrain-mlogloss:0.18041\teval-mlogloss:0.22024\n",
      "[179]\ttrain-mlogloss:0.18009\teval-mlogloss:0.22050\n",
      "[180]\ttrain-mlogloss:0.17971\teval-mlogloss:0.22063\n",
      "[181]\ttrain-mlogloss:0.17938\teval-mlogloss:0.22065\n",
      "[182]\ttrain-mlogloss:0.17910\teval-mlogloss:0.22052\n",
      "[183]\ttrain-mlogloss:0.17868\teval-mlogloss:0.22028\n",
      "[184]\ttrain-mlogloss:0.17838\teval-mlogloss:0.22016\n",
      "[185]\ttrain-mlogloss:0.17804\teval-mlogloss:0.22015\n",
      "[186]\ttrain-mlogloss:0.17771\teval-mlogloss:0.21998\n",
      "[187]\ttrain-mlogloss:0.17733\teval-mlogloss:0.21992\n",
      "[188]\ttrain-mlogloss:0.17694\teval-mlogloss:0.21993\n",
      "[189]\ttrain-mlogloss:0.17656\teval-mlogloss:0.21996\n",
      "[190]\ttrain-mlogloss:0.17619\teval-mlogloss:0.22007\n",
      "[191]\ttrain-mlogloss:0.17581\teval-mlogloss:0.22007\n",
      "[192]\ttrain-mlogloss:0.17553\teval-mlogloss:0.21998\n",
      "[193]\ttrain-mlogloss:0.17527\teval-mlogloss:0.21990\n",
      "[194]\ttrain-mlogloss:0.17481\teval-mlogloss:0.21980\n",
      "[195]\ttrain-mlogloss:0.17449\teval-mlogloss:0.21995\n",
      "[196]\ttrain-mlogloss:0.17419\teval-mlogloss:0.21975\n",
      "[197]\ttrain-mlogloss:0.17381\teval-mlogloss:0.21982\n",
      "[198]\ttrain-mlogloss:0.17351\teval-mlogloss:0.21990\n",
      "[199]\ttrain-mlogloss:0.17318\teval-mlogloss:0.21977\n",
      "[200]\ttrain-mlogloss:0.17295\teval-mlogloss:0.21972\n",
      "[201]\ttrain-mlogloss:0.17247\teval-mlogloss:0.21964\n",
      "[202]\ttrain-mlogloss:0.17219\teval-mlogloss:0.21979\n",
      "[203]\ttrain-mlogloss:0.17194\teval-mlogloss:0.21986\n",
      "[204]\ttrain-mlogloss:0.17165\teval-mlogloss:0.21970\n",
      "[205]\ttrain-mlogloss:0.17137\teval-mlogloss:0.21949\n",
      "[206]\ttrain-mlogloss:0.17111\teval-mlogloss:0.21938\n",
      "[207]\ttrain-mlogloss:0.17079\teval-mlogloss:0.21932\n",
      "[208]\ttrain-mlogloss:0.17036\teval-mlogloss:0.21925\n",
      "[209]\ttrain-mlogloss:0.17009\teval-mlogloss:0.21913\n",
      "[210]\ttrain-mlogloss:0.16967\teval-mlogloss:0.21936\n",
      "[211]\ttrain-mlogloss:0.16923\teval-mlogloss:0.21935\n",
      "[212]\ttrain-mlogloss:0.16898\teval-mlogloss:0.21937\n",
      "[213]\ttrain-mlogloss:0.16867\teval-mlogloss:0.21949\n",
      "[214]\ttrain-mlogloss:0.16838\teval-mlogloss:0.21951\n",
      "[215]\ttrain-mlogloss:0.16814\teval-mlogloss:0.21950\n",
      "[216]\ttrain-mlogloss:0.16790\teval-mlogloss:0.21962\n",
      "[217]\ttrain-mlogloss:0.16749\teval-mlogloss:0.21965\n",
      "[218]\ttrain-mlogloss:0.16724\teval-mlogloss:0.21935\n",
      "[219]\ttrain-mlogloss:0.16700\teval-mlogloss:0.21939\n",
      "[220]\ttrain-mlogloss:0.16672\teval-mlogloss:0.21929\n",
      "[221]\ttrain-mlogloss:0.16643\teval-mlogloss:0.21934\n",
      "[222]\ttrain-mlogloss:0.16611\teval-mlogloss:0.21938\n",
      "[223]\ttrain-mlogloss:0.16578\teval-mlogloss:0.21954\n",
      "[224]\ttrain-mlogloss:0.16560\teval-mlogloss:0.21957\n",
      "[225]\ttrain-mlogloss:0.16529\teval-mlogloss:0.21956\n",
      "[226]\ttrain-mlogloss:0.16493\teval-mlogloss:0.21944\n",
      "[227]\ttrain-mlogloss:0.16464\teval-mlogloss:0.21943\n",
      "[228]\ttrain-mlogloss:0.16440\teval-mlogloss:0.21959\n",
      "[229]\ttrain-mlogloss:0.16406\teval-mlogloss:0.21949\n",
      "[230]\ttrain-mlogloss:0.16377\teval-mlogloss:0.21953\n",
      "[231]\ttrain-mlogloss:0.16356\teval-mlogloss:0.21944\n",
      "[232]\ttrain-mlogloss:0.16323\teval-mlogloss:0.21940\n",
      "[233]\ttrain-mlogloss:0.16299\teval-mlogloss:0.21950\n",
      "[234]\ttrain-mlogloss:0.16271\teval-mlogloss:0.21949\n",
      "[235]\ttrain-mlogloss:0.16249\teval-mlogloss:0.21957\n",
      "[236]\ttrain-mlogloss:0.16225\teval-mlogloss:0.21948\n",
      "[237]\ttrain-mlogloss:0.16188\teval-mlogloss:0.21919\n",
      "[238]\ttrain-mlogloss:0.16166\teval-mlogloss:0.21922\n",
      "[239]\ttrain-mlogloss:0.16147\teval-mlogloss:0.21923\n",
      "[240]\ttrain-mlogloss:0.16116\teval-mlogloss:0.21931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241]\ttrain-mlogloss:0.16098\teval-mlogloss:0.21940\n",
      "[242]\ttrain-mlogloss:0.16079\teval-mlogloss:0.21935\n",
      "[243]\ttrain-mlogloss:0.16055\teval-mlogloss:0.21919\n",
      "[244]\ttrain-mlogloss:0.16037\teval-mlogloss:0.21934\n",
      "[245]\ttrain-mlogloss:0.16005\teval-mlogloss:0.21943\n",
      "[246]\ttrain-mlogloss:0.15985\teval-mlogloss:0.21933\n",
      "[247]\ttrain-mlogloss:0.15962\teval-mlogloss:0.21931\n",
      "[248]\ttrain-mlogloss:0.15941\teval-mlogloss:0.21938\n",
      "[249]\ttrain-mlogloss:0.15916\teval-mlogloss:0.21933\n",
      "[250]\ttrain-mlogloss:0.15897\teval-mlogloss:0.21947\n",
      "[251]\ttrain-mlogloss:0.15866\teval-mlogloss:0.21941\n",
      "[252]\ttrain-mlogloss:0.15845\teval-mlogloss:0.21944\n",
      "[253]\ttrain-mlogloss:0.15820\teval-mlogloss:0.21958\n",
      "[254]\ttrain-mlogloss:0.15800\teval-mlogloss:0.21965\n",
      "[255]\ttrain-mlogloss:0.15768\teval-mlogloss:0.21967\n",
      "[256]\ttrain-mlogloss:0.15749\teval-mlogloss:0.21958\n",
      "[257]\ttrain-mlogloss:0.15727\teval-mlogloss:0.21973\n",
      "[258]\ttrain-mlogloss:0.15708\teval-mlogloss:0.21970\n",
      "[259]\ttrain-mlogloss:0.15695\teval-mlogloss:0.21968\n",
      "[260]\ttrain-mlogloss:0.15666\teval-mlogloss:0.21974\n",
      "[261]\ttrain-mlogloss:0.15640\teval-mlogloss:0.21987\n",
      "[262]\ttrain-mlogloss:0.15618\teval-mlogloss:0.21983\n",
      "[263]\ttrain-mlogloss:0.15600\teval-mlogloss:0.21977\n",
      "[264]\ttrain-mlogloss:0.15584\teval-mlogloss:0.21964\n",
      "[265]\ttrain-mlogloss:0.15563\teval-mlogloss:0.21938\n",
      "[266]\ttrain-mlogloss:0.15543\teval-mlogloss:0.21950\n",
      "[267]\ttrain-mlogloss:0.15529\teval-mlogloss:0.21949\n",
      "[268]\ttrain-mlogloss:0.15503\teval-mlogloss:0.21950\n",
      "[269]\ttrain-mlogloss:0.15477\teval-mlogloss:0.21951\n",
      "[270]\ttrain-mlogloss:0.15457\teval-mlogloss:0.21959\n",
      "[271]\ttrain-mlogloss:0.15429\teval-mlogloss:0.21937\n",
      "[272]\ttrain-mlogloss:0.15413\teval-mlogloss:0.21937\n",
      "[273]\ttrain-mlogloss:0.15397\teval-mlogloss:0.21931\n",
      "[274]\ttrain-mlogloss:0.15383\teval-mlogloss:0.21941\n",
      "[275]\ttrain-mlogloss:0.15363\teval-mlogloss:0.21946\n",
      "[276]\ttrain-mlogloss:0.15345\teval-mlogloss:0.21950\n",
      "[277]\ttrain-mlogloss:0.15321\teval-mlogloss:0.21960\n",
      "[278]\ttrain-mlogloss:0.15308\teval-mlogloss:0.21953\n",
      "[279]\ttrain-mlogloss:0.15286\teval-mlogloss:0.21940\n",
      "[280]\ttrain-mlogloss:0.15248\teval-mlogloss:0.21950\n",
      "[281]\ttrain-mlogloss:0.15215\teval-mlogloss:0.21945\n",
      "[282]\ttrain-mlogloss:0.15195\teval-mlogloss:0.21932\n",
      "[283]\ttrain-mlogloss:0.15180\teval-mlogloss:0.21957\n",
      "[284]\ttrain-mlogloss:0.15170\teval-mlogloss:0.21955\n",
      "[285]\ttrain-mlogloss:0.15143\teval-mlogloss:0.21959\n",
      "[286]\ttrain-mlogloss:0.15124\teval-mlogloss:0.21959\n",
      "[287]\ttrain-mlogloss:0.15102\teval-mlogloss:0.21944\n",
      "[288]\ttrain-mlogloss:0.15084\teval-mlogloss:0.21962\n",
      "[289]\ttrain-mlogloss:0.15077\teval-mlogloss:0.21960\n",
      "[290]\ttrain-mlogloss:0.15055\teval-mlogloss:0.21965\n",
      "[291]\ttrain-mlogloss:0.15033\teval-mlogloss:0.21964\n",
      "[292]\ttrain-mlogloss:0.15020\teval-mlogloss:0.21968\n",
      "[293]\ttrain-mlogloss:0.14999\teval-mlogloss:0.21985\n",
      "[294]\ttrain-mlogloss:0.14971\teval-mlogloss:0.21989\n",
      "[295]\ttrain-mlogloss:0.14950\teval-mlogloss:0.21979\n",
      "[296]\ttrain-mlogloss:0.14928\teval-mlogloss:0.21977\n",
      "[297]\ttrain-mlogloss:0.14908\teval-mlogloss:0.21981\n",
      "[298]\ttrain-mlogloss:0.14897\teval-mlogloss:0.21985\n",
      "[299]\ttrain-mlogloss:0.14874\teval-mlogloss:0.21979\n",
      "[300]\ttrain-mlogloss:0.14859\teval-mlogloss:0.21989\n",
      "[301]\ttrain-mlogloss:0.14853\teval-mlogloss:0.21997\n",
      "[302]\ttrain-mlogloss:0.14844\teval-mlogloss:0.22008\n",
      "[303]\ttrain-mlogloss:0.14825\teval-mlogloss:0.22025\n",
      "[304]\ttrain-mlogloss:0.14810\teval-mlogloss:0.22020\n",
      "[305]\ttrain-mlogloss:0.14797\teval-mlogloss:0.22029\n",
      "[306]\ttrain-mlogloss:0.14771\teval-mlogloss:0.22023\n",
      "[307]\ttrain-mlogloss:0.14746\teval-mlogloss:0.22041\n",
      "[308]\ttrain-mlogloss:0.14732\teval-mlogloss:0.22039\n",
      "[309]\ttrain-mlogloss:0.14711\teval-mlogloss:0.22045\n",
      "xgb now score is: [2.2251069462671875, 2.7018406702484934]\n",
      "[0]\ttrain-mlogloss:0.67120\teval-mlogloss:0.67110\n",
      "[1]\ttrain-mlogloss:0.65086\teval-mlogloss:0.65026\n",
      "[2]\ttrain-mlogloss:0.63148\teval-mlogloss:0.63030\n",
      "[3]\ttrain-mlogloss:0.61315\teval-mlogloss:0.61186\n",
      "[4]\ttrain-mlogloss:0.59590\teval-mlogloss:0.59425\n",
      "[5]\ttrain-mlogloss:0.57972\teval-mlogloss:0.57796\n",
      "[6]\ttrain-mlogloss:0.56398\teval-mlogloss:0.56200\n",
      "[7]\ttrain-mlogloss:0.54927\teval-mlogloss:0.54712\n",
      "[8]\ttrain-mlogloss:0.53534\teval-mlogloss:0.53311\n",
      "[9]\ttrain-mlogloss:0.52224\teval-mlogloss:0.51976\n",
      "[10]\ttrain-mlogloss:0.50960\teval-mlogloss:0.50690\n",
      "[11]\ttrain-mlogloss:0.49743\teval-mlogloss:0.49454\n",
      "[12]\ttrain-mlogloss:0.48588\teval-mlogloss:0.48278\n",
      "[13]\ttrain-mlogloss:0.47482\teval-mlogloss:0.47153\n",
      "[14]\ttrain-mlogloss:0.46442\teval-mlogloss:0.46090\n",
      "[15]\ttrain-mlogloss:0.45462\teval-mlogloss:0.45095\n",
      "[16]\ttrain-mlogloss:0.44522\teval-mlogloss:0.44126\n",
      "[17]\ttrain-mlogloss:0.43618\teval-mlogloss:0.43227\n",
      "[18]\ttrain-mlogloss:0.42753\teval-mlogloss:0.42373\n",
      "[19]\ttrain-mlogloss:0.41922\teval-mlogloss:0.41544\n",
      "[20]\ttrain-mlogloss:0.41132\teval-mlogloss:0.40752\n",
      "[21]\ttrain-mlogloss:0.40379\teval-mlogloss:0.39992\n",
      "[22]\ttrain-mlogloss:0.39649\teval-mlogloss:0.39253\n",
      "[23]\ttrain-mlogloss:0.38941\teval-mlogloss:0.38544\n",
      "[24]\ttrain-mlogloss:0.38287\teval-mlogloss:0.37872\n",
      "[25]\ttrain-mlogloss:0.37651\teval-mlogloss:0.37249\n",
      "[26]\ttrain-mlogloss:0.37044\teval-mlogloss:0.36637\n",
      "[27]\ttrain-mlogloss:0.36468\teval-mlogloss:0.36065\n",
      "[28]\ttrain-mlogloss:0.35911\teval-mlogloss:0.35498\n",
      "[29]\ttrain-mlogloss:0.35371\teval-mlogloss:0.34952\n",
      "[30]\ttrain-mlogloss:0.34849\teval-mlogloss:0.34433\n",
      "[31]\ttrain-mlogloss:0.34352\teval-mlogloss:0.33938\n",
      "[32]\ttrain-mlogloss:0.33891\teval-mlogloss:0.33477\n",
      "[33]\ttrain-mlogloss:0.33435\teval-mlogloss:0.33011\n",
      "[34]\ttrain-mlogloss:0.32998\teval-mlogloss:0.32570\n",
      "[35]\ttrain-mlogloss:0.32588\teval-mlogloss:0.32157\n",
      "[36]\ttrain-mlogloss:0.32187\teval-mlogloss:0.31774\n",
      "[37]\ttrain-mlogloss:0.31804\teval-mlogloss:0.31393\n",
      "[38]\ttrain-mlogloss:0.31420\teval-mlogloss:0.31051\n",
      "[39]\ttrain-mlogloss:0.31052\teval-mlogloss:0.30693\n",
      "[40]\ttrain-mlogloss:0.30708\teval-mlogloss:0.30355\n",
      "[41]\ttrain-mlogloss:0.30374\teval-mlogloss:0.30031\n",
      "[42]\ttrain-mlogloss:0.30051\teval-mlogloss:0.29730\n",
      "[43]\ttrain-mlogloss:0.29740\teval-mlogloss:0.29429\n",
      "[44]\ttrain-mlogloss:0.29454\teval-mlogloss:0.29144\n",
      "[45]\ttrain-mlogloss:0.29161\teval-mlogloss:0.28867\n",
      "[46]\ttrain-mlogloss:0.28895\teval-mlogloss:0.28616\n",
      "[47]\ttrain-mlogloss:0.28642\teval-mlogloss:0.28365\n",
      "[48]\ttrain-mlogloss:0.28388\teval-mlogloss:0.28145\n",
      "[49]\ttrain-mlogloss:0.28125\teval-mlogloss:0.27890\n",
      "[50]\ttrain-mlogloss:0.27894\teval-mlogloss:0.27664\n",
      "[51]\ttrain-mlogloss:0.27669\teval-mlogloss:0.27465\n",
      "[52]\ttrain-mlogloss:0.27436\teval-mlogloss:0.27278\n",
      "[53]\ttrain-mlogloss:0.27227\teval-mlogloss:0.27080\n",
      "[54]\ttrain-mlogloss:0.27015\teval-mlogloss:0.26908\n",
      "[55]\ttrain-mlogloss:0.26791\teval-mlogloss:0.26724\n",
      "[56]\ttrain-mlogloss:0.26591\teval-mlogloss:0.26553\n",
      "[57]\ttrain-mlogloss:0.26395\teval-mlogloss:0.26403\n",
      "[58]\ttrain-mlogloss:0.26218\teval-mlogloss:0.26253\n",
      "[59]\ttrain-mlogloss:0.26034\teval-mlogloss:0.26116\n",
      "[60]\ttrain-mlogloss:0.25850\teval-mlogloss:0.25988\n",
      "[61]\ttrain-mlogloss:0.25678\teval-mlogloss:0.25869\n",
      "[62]\ttrain-mlogloss:0.25510\teval-mlogloss:0.25753\n",
      "[63]\ttrain-mlogloss:0.25352\teval-mlogloss:0.25668\n",
      "[64]\ttrain-mlogloss:0.25184\teval-mlogloss:0.25553\n",
      "[65]\ttrain-mlogloss:0.25032\teval-mlogloss:0.25427\n",
      "[66]\ttrain-mlogloss:0.24873\teval-mlogloss:0.25309\n",
      "[67]\ttrain-mlogloss:0.24754\teval-mlogloss:0.25203\n",
      "[68]\ttrain-mlogloss:0.24607\teval-mlogloss:0.25117\n",
      "[69]\ttrain-mlogloss:0.24484\teval-mlogloss:0.25021\n",
      "[70]\ttrain-mlogloss:0.24341\teval-mlogloss:0.24923\n",
      "[71]\ttrain-mlogloss:0.24214\teval-mlogloss:0.24847\n",
      "[72]\ttrain-mlogloss:0.24099\teval-mlogloss:0.24781\n",
      "[73]\ttrain-mlogloss:0.23967\teval-mlogloss:0.24710\n",
      "[74]\ttrain-mlogloss:0.23833\teval-mlogloss:0.24648\n",
      "[75]\ttrain-mlogloss:0.23719\teval-mlogloss:0.24579\n",
      "[76]\ttrain-mlogloss:0.23606\teval-mlogloss:0.24503\n",
      "[77]\ttrain-mlogloss:0.23489\teval-mlogloss:0.24446\n",
      "[78]\ttrain-mlogloss:0.23386\teval-mlogloss:0.24382\n",
      "[79]\ttrain-mlogloss:0.23266\teval-mlogloss:0.24313\n",
      "[80]\ttrain-mlogloss:0.23142\teval-mlogloss:0.24266\n",
      "[81]\ttrain-mlogloss:0.23051\teval-mlogloss:0.24207\n",
      "[82]\ttrain-mlogloss:0.22939\teval-mlogloss:0.24172\n",
      "[83]\ttrain-mlogloss:0.22834\teval-mlogloss:0.24114\n",
      "[84]\ttrain-mlogloss:0.22734\teval-mlogloss:0.24077\n",
      "[85]\ttrain-mlogloss:0.22641\teval-mlogloss:0.24044\n",
      "[86]\ttrain-mlogloss:0.22541\teval-mlogloss:0.24040\n",
      "[87]\ttrain-mlogloss:0.22452\teval-mlogloss:0.23997\n",
      "[88]\ttrain-mlogloss:0.22367\teval-mlogloss:0.23970\n",
      "[89]\ttrain-mlogloss:0.22283\teval-mlogloss:0.23942\n",
      "[90]\ttrain-mlogloss:0.22195\teval-mlogloss:0.23910\n",
      "[91]\ttrain-mlogloss:0.22120\teval-mlogloss:0.23882\n",
      "[92]\ttrain-mlogloss:0.22043\teval-mlogloss:0.23846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93]\ttrain-mlogloss:0.21964\teval-mlogloss:0.23819\n",
      "[94]\ttrain-mlogloss:0.21879\teval-mlogloss:0.23790\n",
      "[95]\ttrain-mlogloss:0.21798\teval-mlogloss:0.23767\n",
      "[96]\ttrain-mlogloss:0.21725\teval-mlogloss:0.23749\n",
      "[97]\ttrain-mlogloss:0.21646\teval-mlogloss:0.23715\n",
      "[98]\ttrain-mlogloss:0.21575\teval-mlogloss:0.23708\n",
      "[99]\ttrain-mlogloss:0.21508\teval-mlogloss:0.23688\n",
      "[100]\ttrain-mlogloss:0.21448\teval-mlogloss:0.23681\n",
      "[101]\ttrain-mlogloss:0.21384\teval-mlogloss:0.23663\n",
      "[102]\ttrain-mlogloss:0.21308\teval-mlogloss:0.23649\n",
      "[103]\ttrain-mlogloss:0.21221\teval-mlogloss:0.23632\n",
      "[104]\ttrain-mlogloss:0.21141\teval-mlogloss:0.23627\n",
      "[105]\ttrain-mlogloss:0.21054\teval-mlogloss:0.23603\n",
      "[106]\ttrain-mlogloss:0.20994\teval-mlogloss:0.23591\n",
      "[107]\ttrain-mlogloss:0.20922\teval-mlogloss:0.23580\n",
      "[108]\ttrain-mlogloss:0.20862\teval-mlogloss:0.23578\n",
      "[109]\ttrain-mlogloss:0.20805\teval-mlogloss:0.23573\n",
      "[110]\ttrain-mlogloss:0.20720\teval-mlogloss:0.23565\n",
      "[111]\ttrain-mlogloss:0.20667\teval-mlogloss:0.23564\n",
      "[112]\ttrain-mlogloss:0.20604\teval-mlogloss:0.23538\n",
      "[113]\ttrain-mlogloss:0.20536\teval-mlogloss:0.23540\n",
      "[114]\ttrain-mlogloss:0.20483\teval-mlogloss:0.23532\n",
      "[115]\ttrain-mlogloss:0.20414\teval-mlogloss:0.23533\n",
      "[116]\ttrain-mlogloss:0.20344\teval-mlogloss:0.23562\n",
      "[117]\ttrain-mlogloss:0.20295\teval-mlogloss:0.23559\n",
      "[118]\ttrain-mlogloss:0.20245\teval-mlogloss:0.23554\n",
      "[119]\ttrain-mlogloss:0.20177\teval-mlogloss:0.23561\n",
      "[120]\ttrain-mlogloss:0.20113\teval-mlogloss:0.23552\n",
      "[121]\ttrain-mlogloss:0.20061\teval-mlogloss:0.23538\n",
      "[122]\ttrain-mlogloss:0.20008\teval-mlogloss:0.23549\n",
      "[123]\ttrain-mlogloss:0.19953\teval-mlogloss:0.23545\n",
      "[124]\ttrain-mlogloss:0.19907\teval-mlogloss:0.23541\n",
      "[125]\ttrain-mlogloss:0.19852\teval-mlogloss:0.23541\n",
      "[126]\ttrain-mlogloss:0.19789\teval-mlogloss:0.23534\n",
      "[127]\ttrain-mlogloss:0.19746\teval-mlogloss:0.23538\n",
      "[128]\ttrain-mlogloss:0.19689\teval-mlogloss:0.23534\n",
      "[129]\ttrain-mlogloss:0.19623\teval-mlogloss:0.23525\n",
      "[130]\ttrain-mlogloss:0.19563\teval-mlogloss:0.23518\n",
      "[131]\ttrain-mlogloss:0.19501\teval-mlogloss:0.23506\n",
      "[132]\ttrain-mlogloss:0.19434\teval-mlogloss:0.23492\n",
      "[133]\ttrain-mlogloss:0.19388\teval-mlogloss:0.23513\n",
      "[134]\ttrain-mlogloss:0.19337\teval-mlogloss:0.23494\n",
      "[135]\ttrain-mlogloss:0.19299\teval-mlogloss:0.23498\n",
      "[136]\ttrain-mlogloss:0.19235\teval-mlogloss:0.23509\n",
      "[137]\ttrain-mlogloss:0.19197\teval-mlogloss:0.23506\n",
      "[138]\ttrain-mlogloss:0.19126\teval-mlogloss:0.23525\n",
      "[139]\ttrain-mlogloss:0.19080\teval-mlogloss:0.23541\n",
      "[140]\ttrain-mlogloss:0.19039\teval-mlogloss:0.23534\n",
      "[141]\ttrain-mlogloss:0.18993\teval-mlogloss:0.23548\n",
      "[142]\ttrain-mlogloss:0.18941\teval-mlogloss:0.23540\n",
      "[143]\ttrain-mlogloss:0.18887\teval-mlogloss:0.23542\n",
      "[144]\ttrain-mlogloss:0.18838\teval-mlogloss:0.23529\n",
      "[145]\ttrain-mlogloss:0.18789\teval-mlogloss:0.23541\n",
      "[146]\ttrain-mlogloss:0.18751\teval-mlogloss:0.23544\n",
      "[147]\ttrain-mlogloss:0.18709\teval-mlogloss:0.23545\n",
      "[148]\ttrain-mlogloss:0.18678\teval-mlogloss:0.23548\n",
      "[149]\ttrain-mlogloss:0.18611\teval-mlogloss:0.23559\n",
      "[150]\ttrain-mlogloss:0.18557\teval-mlogloss:0.23570\n",
      "[151]\ttrain-mlogloss:0.18519\teval-mlogloss:0.23579\n",
      "[152]\ttrain-mlogloss:0.18461\teval-mlogloss:0.23572\n",
      "[153]\ttrain-mlogloss:0.18428\teval-mlogloss:0.23580\n",
      "[154]\ttrain-mlogloss:0.18378\teval-mlogloss:0.23589\n",
      "[155]\ttrain-mlogloss:0.18335\teval-mlogloss:0.23592\n",
      "[156]\ttrain-mlogloss:0.18303\teval-mlogloss:0.23591\n",
      "[157]\ttrain-mlogloss:0.18269\teval-mlogloss:0.23590\n",
      "[158]\ttrain-mlogloss:0.18228\teval-mlogloss:0.23595\n",
      "[159]\ttrain-mlogloss:0.18186\teval-mlogloss:0.23607\n",
      "[160]\ttrain-mlogloss:0.18132\teval-mlogloss:0.23628\n",
      "[161]\ttrain-mlogloss:0.18103\teval-mlogloss:0.23640\n",
      "[162]\ttrain-mlogloss:0.18082\teval-mlogloss:0.23629\n",
      "[163]\ttrain-mlogloss:0.18039\teval-mlogloss:0.23620\n",
      "[164]\ttrain-mlogloss:0.18004\teval-mlogloss:0.23610\n",
      "[165]\ttrain-mlogloss:0.17964\teval-mlogloss:0.23624\n",
      "[166]\ttrain-mlogloss:0.17923\teval-mlogloss:0.23631\n",
      "[167]\ttrain-mlogloss:0.17880\teval-mlogloss:0.23631\n",
      "[168]\ttrain-mlogloss:0.17845\teval-mlogloss:0.23620\n",
      "[169]\ttrain-mlogloss:0.17798\teval-mlogloss:0.23632\n",
      "[170]\ttrain-mlogloss:0.17762\teval-mlogloss:0.23629\n",
      "[171]\ttrain-mlogloss:0.17720\teval-mlogloss:0.23658\n",
      "[172]\ttrain-mlogloss:0.17680\teval-mlogloss:0.23666\n",
      "[173]\ttrain-mlogloss:0.17637\teval-mlogloss:0.23684\n",
      "[174]\ttrain-mlogloss:0.17590\teval-mlogloss:0.23686\n",
      "[175]\ttrain-mlogloss:0.17546\teval-mlogloss:0.23710\n",
      "[176]\ttrain-mlogloss:0.17505\teval-mlogloss:0.23723\n",
      "[177]\ttrain-mlogloss:0.17477\teval-mlogloss:0.23731\n",
      "[178]\ttrain-mlogloss:0.17444\teval-mlogloss:0.23731\n",
      "[179]\ttrain-mlogloss:0.17406\teval-mlogloss:0.23737\n",
      "[180]\ttrain-mlogloss:0.17373\teval-mlogloss:0.23752\n",
      "[181]\ttrain-mlogloss:0.17335\teval-mlogloss:0.23764\n",
      "[182]\ttrain-mlogloss:0.17296\teval-mlogloss:0.23775\n",
      "[183]\ttrain-mlogloss:0.17264\teval-mlogloss:0.23794\n",
      "[184]\ttrain-mlogloss:0.17238\teval-mlogloss:0.23794\n",
      "[185]\ttrain-mlogloss:0.17200\teval-mlogloss:0.23809\n",
      "[186]\ttrain-mlogloss:0.17166\teval-mlogloss:0.23811\n",
      "[187]\ttrain-mlogloss:0.17146\teval-mlogloss:0.23819\n",
      "[188]\ttrain-mlogloss:0.17112\teval-mlogloss:0.23805\n",
      "[189]\ttrain-mlogloss:0.17083\teval-mlogloss:0.23789\n",
      "[190]\ttrain-mlogloss:0.17046\teval-mlogloss:0.23775\n",
      "[191]\ttrain-mlogloss:0.17019\teval-mlogloss:0.23788\n",
      "[192]\ttrain-mlogloss:0.16982\teval-mlogloss:0.23776\n",
      "[193]\ttrain-mlogloss:0.16952\teval-mlogloss:0.23784\n",
      "[194]\ttrain-mlogloss:0.16928\teval-mlogloss:0.23789\n",
      "[195]\ttrain-mlogloss:0.16901\teval-mlogloss:0.23792\n",
      "[196]\ttrain-mlogloss:0.16874\teval-mlogloss:0.23792\n",
      "[197]\ttrain-mlogloss:0.16834\teval-mlogloss:0.23825\n",
      "[198]\ttrain-mlogloss:0.16799\teval-mlogloss:0.23820\n",
      "[199]\ttrain-mlogloss:0.16762\teval-mlogloss:0.23850\n",
      "[200]\ttrain-mlogloss:0.16730\teval-mlogloss:0.23836\n",
      "[201]\ttrain-mlogloss:0.16687\teval-mlogloss:0.23848\n",
      "[202]\ttrain-mlogloss:0.16654\teval-mlogloss:0.23857\n",
      "[203]\ttrain-mlogloss:0.16627\teval-mlogloss:0.23877\n",
      "[204]\ttrain-mlogloss:0.16601\teval-mlogloss:0.23882\n",
      "[205]\ttrain-mlogloss:0.16556\teval-mlogloss:0.23894\n",
      "[206]\ttrain-mlogloss:0.16521\teval-mlogloss:0.23906\n",
      "[207]\ttrain-mlogloss:0.16476\teval-mlogloss:0.23935\n",
      "[208]\ttrain-mlogloss:0.16446\teval-mlogloss:0.23952\n",
      "[209]\ttrain-mlogloss:0.16426\teval-mlogloss:0.23944\n",
      "[210]\ttrain-mlogloss:0.16391\teval-mlogloss:0.23959\n",
      "[211]\ttrain-mlogloss:0.16366\teval-mlogloss:0.23970\n",
      "[212]\ttrain-mlogloss:0.16339\teval-mlogloss:0.23978\n",
      "[213]\ttrain-mlogloss:0.16309\teval-mlogloss:0.23982\n",
      "[214]\ttrain-mlogloss:0.16290\teval-mlogloss:0.23986\n",
      "[215]\ttrain-mlogloss:0.16271\teval-mlogloss:0.23995\n",
      "[216]\ttrain-mlogloss:0.16248\teval-mlogloss:0.23980\n",
      "[217]\ttrain-mlogloss:0.16210\teval-mlogloss:0.24007\n",
      "[218]\ttrain-mlogloss:0.16177\teval-mlogloss:0.24000\n",
      "[219]\ttrain-mlogloss:0.16156\teval-mlogloss:0.24009\n",
      "[220]\ttrain-mlogloss:0.16123\teval-mlogloss:0.24019\n",
      "[221]\ttrain-mlogloss:0.16093\teval-mlogloss:0.24021\n",
      "[222]\ttrain-mlogloss:0.16050\teval-mlogloss:0.24028\n",
      "[223]\ttrain-mlogloss:0.16025\teval-mlogloss:0.24023\n",
      "[224]\ttrain-mlogloss:0.15998\teval-mlogloss:0.24020\n",
      "[225]\ttrain-mlogloss:0.15966\teval-mlogloss:0.24033\n",
      "[226]\ttrain-mlogloss:0.15937\teval-mlogloss:0.24043\n",
      "[227]\ttrain-mlogloss:0.15912\teval-mlogloss:0.24055\n",
      "[228]\ttrain-mlogloss:0.15901\teval-mlogloss:0.24065\n",
      "[229]\ttrain-mlogloss:0.15861\teval-mlogloss:0.24088\n",
      "[230]\ttrain-mlogloss:0.15828\teval-mlogloss:0.24091\n",
      "[231]\ttrain-mlogloss:0.15813\teval-mlogloss:0.24101\n",
      "[232]\ttrain-mlogloss:0.15777\teval-mlogloss:0.24132\n",
      "xgb now score is: [2.2251069462671875, 2.7018406702484934, 2.5559553388599308]\n",
      "[0]\ttrain-mlogloss:0.67075\teval-mlogloss:0.67190\n",
      "[1]\ttrain-mlogloss:0.64964\teval-mlogloss:0.65261\n",
      "[2]\ttrain-mlogloss:0.62981\teval-mlogloss:0.63399\n",
      "[3]\ttrain-mlogloss:0.61119\teval-mlogloss:0.61654\n",
      "[4]\ttrain-mlogloss:0.59356\teval-mlogloss:0.60022\n",
      "[5]\ttrain-mlogloss:0.57706\teval-mlogloss:0.58482\n",
      "[6]\ttrain-mlogloss:0.56124\teval-mlogloss:0.57015\n",
      "[7]\ttrain-mlogloss:0.54608\teval-mlogloss:0.55626\n",
      "[8]\ttrain-mlogloss:0.53156\teval-mlogloss:0.54279\n",
      "[9]\ttrain-mlogloss:0.51795\teval-mlogloss:0.53016\n",
      "[10]\ttrain-mlogloss:0.50510\teval-mlogloss:0.51838\n",
      "[11]\ttrain-mlogloss:0.49280\teval-mlogloss:0.50698\n",
      "[12]\ttrain-mlogloss:0.48084\teval-mlogloss:0.49627\n",
      "[13]\ttrain-mlogloss:0.46942\teval-mlogloss:0.48602\n",
      "[14]\ttrain-mlogloss:0.45873\teval-mlogloss:0.47633\n",
      "[15]\ttrain-mlogloss:0.44854\teval-mlogloss:0.46711\n",
      "[16]\ttrain-mlogloss:0.43882\teval-mlogloss:0.45842\n",
      "[17]\ttrain-mlogloss:0.42938\teval-mlogloss:0.45010\n",
      "[18]\ttrain-mlogloss:0.42033\teval-mlogloss:0.44210\n",
      "[19]\ttrain-mlogloss:0.41174\teval-mlogloss:0.43428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttrain-mlogloss:0.40351\teval-mlogloss:0.42691\n",
      "[21]\ttrain-mlogloss:0.39582\teval-mlogloss:0.42007\n",
      "[22]\ttrain-mlogloss:0.38845\teval-mlogloss:0.41369\n",
      "[23]\ttrain-mlogloss:0.38135\teval-mlogloss:0.40765\n",
      "[24]\ttrain-mlogloss:0.37447\teval-mlogloss:0.40168\n",
      "[25]\ttrain-mlogloss:0.36793\teval-mlogloss:0.39611\n",
      "[26]\ttrain-mlogloss:0.36160\teval-mlogloss:0.39064\n",
      "[27]\ttrain-mlogloss:0.35553\teval-mlogloss:0.38564\n",
      "[28]\ttrain-mlogloss:0.34969\teval-mlogloss:0.38087\n",
      "[29]\ttrain-mlogloss:0.34411\teval-mlogloss:0.37616\n",
      "[30]\ttrain-mlogloss:0.33896\teval-mlogloss:0.37190\n",
      "[31]\ttrain-mlogloss:0.33389\teval-mlogloss:0.36774\n",
      "[32]\ttrain-mlogloss:0.32911\teval-mlogloss:0.36388\n",
      "[33]\ttrain-mlogloss:0.32439\teval-mlogloss:0.35989\n",
      "[34]\ttrain-mlogloss:0.31988\teval-mlogloss:0.35625\n",
      "[35]\ttrain-mlogloss:0.31554\teval-mlogloss:0.35267\n",
      "[36]\ttrain-mlogloss:0.31141\teval-mlogloss:0.34941\n",
      "[37]\ttrain-mlogloss:0.30740\teval-mlogloss:0.34636\n",
      "[38]\ttrain-mlogloss:0.30350\teval-mlogloss:0.34342\n",
      "[39]\ttrain-mlogloss:0.29983\teval-mlogloss:0.34078\n",
      "[40]\ttrain-mlogloss:0.29628\teval-mlogloss:0.33814\n",
      "[41]\ttrain-mlogloss:0.29279\teval-mlogloss:0.33561\n",
      "[42]\ttrain-mlogloss:0.28952\teval-mlogloss:0.33312\n",
      "[43]\ttrain-mlogloss:0.28628\teval-mlogloss:0.33091\n",
      "[44]\ttrain-mlogloss:0.28314\teval-mlogloss:0.32856\n",
      "[45]\ttrain-mlogloss:0.28017\teval-mlogloss:0.32638\n",
      "[46]\ttrain-mlogloss:0.27743\teval-mlogloss:0.32443\n",
      "[47]\ttrain-mlogloss:0.27467\teval-mlogloss:0.32253\n",
      "[48]\ttrain-mlogloss:0.27198\teval-mlogloss:0.32075\n",
      "[49]\ttrain-mlogloss:0.26939\teval-mlogloss:0.31916\n",
      "[50]\ttrain-mlogloss:0.26698\teval-mlogloss:0.31748\n",
      "[51]\ttrain-mlogloss:0.26459\teval-mlogloss:0.31598\n",
      "[52]\ttrain-mlogloss:0.26237\teval-mlogloss:0.31443\n",
      "[53]\ttrain-mlogloss:0.26016\teval-mlogloss:0.31302\n",
      "[54]\ttrain-mlogloss:0.25801\teval-mlogloss:0.31173\n",
      "[55]\ttrain-mlogloss:0.25593\teval-mlogloss:0.31059\n",
      "[56]\ttrain-mlogloss:0.25389\teval-mlogloss:0.30929\n",
      "[57]\ttrain-mlogloss:0.25207\teval-mlogloss:0.30827\n",
      "[58]\ttrain-mlogloss:0.25023\teval-mlogloss:0.30721\n",
      "[59]\ttrain-mlogloss:0.24840\teval-mlogloss:0.30610\n",
      "[60]\ttrain-mlogloss:0.24661\teval-mlogloss:0.30487\n",
      "[61]\ttrain-mlogloss:0.24492\teval-mlogloss:0.30408\n",
      "[62]\ttrain-mlogloss:0.24325\teval-mlogloss:0.30306\n",
      "[63]\ttrain-mlogloss:0.24177\teval-mlogloss:0.30241\n",
      "[64]\ttrain-mlogloss:0.24042\teval-mlogloss:0.30174\n",
      "[65]\ttrain-mlogloss:0.23903\teval-mlogloss:0.30106\n",
      "[66]\ttrain-mlogloss:0.23754\teval-mlogloss:0.30040\n",
      "[67]\ttrain-mlogloss:0.23619\teval-mlogloss:0.29973\n",
      "[68]\ttrain-mlogloss:0.23494\teval-mlogloss:0.29916\n",
      "[69]\ttrain-mlogloss:0.23367\teval-mlogloss:0.29847\n",
      "[70]\ttrain-mlogloss:0.23231\teval-mlogloss:0.29781\n",
      "[71]\ttrain-mlogloss:0.23118\teval-mlogloss:0.29728\n",
      "[72]\ttrain-mlogloss:0.22999\teval-mlogloss:0.29674\n",
      "[73]\ttrain-mlogloss:0.22892\teval-mlogloss:0.29620\n",
      "[74]\ttrain-mlogloss:0.22780\teval-mlogloss:0.29576\n",
      "[75]\ttrain-mlogloss:0.22683\teval-mlogloss:0.29542\n",
      "[76]\ttrain-mlogloss:0.22586\teval-mlogloss:0.29481\n",
      "[77]\ttrain-mlogloss:0.22499\teval-mlogloss:0.29460\n",
      "[78]\ttrain-mlogloss:0.22397\teval-mlogloss:0.29441\n",
      "[79]\ttrain-mlogloss:0.22298\teval-mlogloss:0.29399\n",
      "[80]\ttrain-mlogloss:0.22209\teval-mlogloss:0.29370\n",
      "[81]\ttrain-mlogloss:0.22140\teval-mlogloss:0.29358\n",
      "[82]\ttrain-mlogloss:0.22070\teval-mlogloss:0.29342\n",
      "[83]\ttrain-mlogloss:0.21984\teval-mlogloss:0.29314\n",
      "[84]\ttrain-mlogloss:0.21874\teval-mlogloss:0.29294\n",
      "[85]\ttrain-mlogloss:0.21783\teval-mlogloss:0.29277\n",
      "[86]\ttrain-mlogloss:0.21690\teval-mlogloss:0.29261\n",
      "[87]\ttrain-mlogloss:0.21598\teval-mlogloss:0.29246\n",
      "[88]\ttrain-mlogloss:0.21528\teval-mlogloss:0.29235\n",
      "[89]\ttrain-mlogloss:0.21446\teval-mlogloss:0.29241\n",
      "[90]\ttrain-mlogloss:0.21367\teval-mlogloss:0.29214\n",
      "[91]\ttrain-mlogloss:0.21289\teval-mlogloss:0.29227\n",
      "[92]\ttrain-mlogloss:0.21220\teval-mlogloss:0.29236\n",
      "[93]\ttrain-mlogloss:0.21144\teval-mlogloss:0.29250\n",
      "[94]\ttrain-mlogloss:0.21067\teval-mlogloss:0.29239\n",
      "[95]\ttrain-mlogloss:0.21008\teval-mlogloss:0.29228\n",
      "[96]\ttrain-mlogloss:0.20943\teval-mlogloss:0.29227\n",
      "[97]\ttrain-mlogloss:0.20882\teval-mlogloss:0.29227\n",
      "[98]\ttrain-mlogloss:0.20813\teval-mlogloss:0.29214\n",
      "[99]\ttrain-mlogloss:0.20758\teval-mlogloss:0.29217\n",
      "[100]\ttrain-mlogloss:0.20691\teval-mlogloss:0.29211\n",
      "[101]\ttrain-mlogloss:0.20639\teval-mlogloss:0.29211\n",
      "[102]\ttrain-mlogloss:0.20569\teval-mlogloss:0.29185\n",
      "[103]\ttrain-mlogloss:0.20510\teval-mlogloss:0.29178\n",
      "[104]\ttrain-mlogloss:0.20439\teval-mlogloss:0.29173\n",
      "[105]\ttrain-mlogloss:0.20391\teval-mlogloss:0.29166\n",
      "[106]\ttrain-mlogloss:0.20328\teval-mlogloss:0.29170\n",
      "[107]\ttrain-mlogloss:0.20256\teval-mlogloss:0.29139\n",
      "[108]\ttrain-mlogloss:0.20208\teval-mlogloss:0.29113\n",
      "[109]\ttrain-mlogloss:0.20157\teval-mlogloss:0.29099\n",
      "[110]\ttrain-mlogloss:0.20106\teval-mlogloss:0.29089\n",
      "[111]\ttrain-mlogloss:0.20048\teval-mlogloss:0.29085\n",
      "[112]\ttrain-mlogloss:0.20001\teval-mlogloss:0.29098\n",
      "[113]\ttrain-mlogloss:0.19931\teval-mlogloss:0.29101\n",
      "[114]\ttrain-mlogloss:0.19884\teval-mlogloss:0.29092\n",
      "[115]\ttrain-mlogloss:0.19833\teval-mlogloss:0.29087\n",
      "[116]\ttrain-mlogloss:0.19787\teval-mlogloss:0.29080\n",
      "[117]\ttrain-mlogloss:0.19744\teval-mlogloss:0.29088\n",
      "[118]\ttrain-mlogloss:0.19690\teval-mlogloss:0.29071\n",
      "[119]\ttrain-mlogloss:0.19630\teval-mlogloss:0.29080\n",
      "[120]\ttrain-mlogloss:0.19578\teval-mlogloss:0.29100\n",
      "[121]\ttrain-mlogloss:0.19525\teval-mlogloss:0.29111\n",
      "[122]\ttrain-mlogloss:0.19469\teval-mlogloss:0.29125\n",
      "[123]\ttrain-mlogloss:0.19415\teval-mlogloss:0.29135\n",
      "[124]\ttrain-mlogloss:0.19370\teval-mlogloss:0.29136\n",
      "[125]\ttrain-mlogloss:0.19324\teval-mlogloss:0.29140\n",
      "[126]\ttrain-mlogloss:0.19272\teval-mlogloss:0.29153\n",
      "[127]\ttrain-mlogloss:0.19226\teval-mlogloss:0.29157\n",
      "[128]\ttrain-mlogloss:0.19173\teval-mlogloss:0.29173\n",
      "[129]\ttrain-mlogloss:0.19142\teval-mlogloss:0.29201\n",
      "[130]\ttrain-mlogloss:0.19092\teval-mlogloss:0.29212\n",
      "[131]\ttrain-mlogloss:0.19040\teval-mlogloss:0.29226\n",
      "[132]\ttrain-mlogloss:0.18994\teval-mlogloss:0.29218\n",
      "[133]\ttrain-mlogloss:0.18946\teval-mlogloss:0.29233\n",
      "[134]\ttrain-mlogloss:0.18899\teval-mlogloss:0.29240\n",
      "[135]\ttrain-mlogloss:0.18855\teval-mlogloss:0.29273\n",
      "[136]\ttrain-mlogloss:0.18803\teval-mlogloss:0.29254\n",
      "[137]\ttrain-mlogloss:0.18748\teval-mlogloss:0.29255\n",
      "[138]\ttrain-mlogloss:0.18714\teval-mlogloss:0.29249\n",
      "[139]\ttrain-mlogloss:0.18666\teval-mlogloss:0.29265\n",
      "[140]\ttrain-mlogloss:0.18626\teval-mlogloss:0.29261\n",
      "[141]\ttrain-mlogloss:0.18593\teval-mlogloss:0.29258\n",
      "[142]\ttrain-mlogloss:0.18544\teval-mlogloss:0.29273\n",
      "[143]\ttrain-mlogloss:0.18499\teval-mlogloss:0.29288\n",
      "[144]\ttrain-mlogloss:0.18458\teval-mlogloss:0.29289\n",
      "[145]\ttrain-mlogloss:0.18405\teval-mlogloss:0.29310\n",
      "[146]\ttrain-mlogloss:0.18365\teval-mlogloss:0.29327\n",
      "[147]\ttrain-mlogloss:0.18328\teval-mlogloss:0.29359\n",
      "[148]\ttrain-mlogloss:0.18296\teval-mlogloss:0.29388\n",
      "[149]\ttrain-mlogloss:0.18254\teval-mlogloss:0.29391\n",
      "[150]\ttrain-mlogloss:0.18201\teval-mlogloss:0.29394\n",
      "[151]\ttrain-mlogloss:0.18166\teval-mlogloss:0.29401\n",
      "[152]\ttrain-mlogloss:0.18142\teval-mlogloss:0.29407\n",
      "[153]\ttrain-mlogloss:0.18103\teval-mlogloss:0.29398\n",
      "[154]\ttrain-mlogloss:0.18069\teval-mlogloss:0.29408\n",
      "[155]\ttrain-mlogloss:0.18029\teval-mlogloss:0.29415\n",
      "[156]\ttrain-mlogloss:0.17988\teval-mlogloss:0.29423\n",
      "[157]\ttrain-mlogloss:0.17947\teval-mlogloss:0.29410\n",
      "[158]\ttrain-mlogloss:0.17915\teval-mlogloss:0.29412\n",
      "[159]\ttrain-mlogloss:0.17882\teval-mlogloss:0.29410\n",
      "[160]\ttrain-mlogloss:0.17839\teval-mlogloss:0.29433\n",
      "[161]\ttrain-mlogloss:0.17804\teval-mlogloss:0.29442\n",
      "[162]\ttrain-mlogloss:0.17758\teval-mlogloss:0.29453\n",
      "[163]\ttrain-mlogloss:0.17715\teval-mlogloss:0.29447\n",
      "[164]\ttrain-mlogloss:0.17680\teval-mlogloss:0.29453\n",
      "[165]\ttrain-mlogloss:0.17645\teval-mlogloss:0.29456\n",
      "[166]\ttrain-mlogloss:0.17616\teval-mlogloss:0.29466\n",
      "[167]\ttrain-mlogloss:0.17586\teval-mlogloss:0.29466\n",
      "[168]\ttrain-mlogloss:0.17563\teval-mlogloss:0.29476\n",
      "[169]\ttrain-mlogloss:0.17528\teval-mlogloss:0.29496\n",
      "[170]\ttrain-mlogloss:0.17492\teval-mlogloss:0.29530\n",
      "[171]\ttrain-mlogloss:0.17453\teval-mlogloss:0.29534\n",
      "[172]\ttrain-mlogloss:0.17421\teval-mlogloss:0.29537\n",
      "[173]\ttrain-mlogloss:0.17389\teval-mlogloss:0.29513\n",
      "[174]\ttrain-mlogloss:0.17342\teval-mlogloss:0.29515\n",
      "[175]\ttrain-mlogloss:0.17312\teval-mlogloss:0.29510\n",
      "[176]\ttrain-mlogloss:0.17271\teval-mlogloss:0.29504\n",
      "[177]\ttrain-mlogloss:0.17233\teval-mlogloss:0.29488\n",
      "[178]\ttrain-mlogloss:0.17206\teval-mlogloss:0.29489\n",
      "[179]\ttrain-mlogloss:0.17172\teval-mlogloss:0.29514\n",
      "[180]\ttrain-mlogloss:0.17134\teval-mlogloss:0.29485\n",
      "[181]\ttrain-mlogloss:0.17097\teval-mlogloss:0.29507\n",
      "[182]\ttrain-mlogloss:0.17061\teval-mlogloss:0.29517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183]\ttrain-mlogloss:0.17029\teval-mlogloss:0.29519\n",
      "[184]\ttrain-mlogloss:0.16997\teval-mlogloss:0.29530\n",
      "[185]\ttrain-mlogloss:0.16963\teval-mlogloss:0.29537\n",
      "[186]\ttrain-mlogloss:0.16938\teval-mlogloss:0.29526\n",
      "[187]\ttrain-mlogloss:0.16911\teval-mlogloss:0.29540\n",
      "[188]\ttrain-mlogloss:0.16880\teval-mlogloss:0.29547\n",
      "[189]\ttrain-mlogloss:0.16852\teval-mlogloss:0.29533\n",
      "[190]\ttrain-mlogloss:0.16812\teval-mlogloss:0.29536\n",
      "[191]\ttrain-mlogloss:0.16776\teval-mlogloss:0.29531\n",
      "[192]\ttrain-mlogloss:0.16749\teval-mlogloss:0.29526\n",
      "[193]\ttrain-mlogloss:0.16711\teval-mlogloss:0.29524\n",
      "[194]\ttrain-mlogloss:0.16688\teval-mlogloss:0.29547\n",
      "[195]\ttrain-mlogloss:0.16661\teval-mlogloss:0.29557\n",
      "[196]\ttrain-mlogloss:0.16640\teval-mlogloss:0.29544\n",
      "[197]\ttrain-mlogloss:0.16622\teval-mlogloss:0.29546\n",
      "[198]\ttrain-mlogloss:0.16595\teval-mlogloss:0.29565\n",
      "[199]\ttrain-mlogloss:0.16567\teval-mlogloss:0.29567\n",
      "[200]\ttrain-mlogloss:0.16534\teval-mlogloss:0.29578\n",
      "[201]\ttrain-mlogloss:0.16518\teval-mlogloss:0.29584\n",
      "[202]\ttrain-mlogloss:0.16487\teval-mlogloss:0.29584\n",
      "[203]\ttrain-mlogloss:0.16453\teval-mlogloss:0.29600\n",
      "[204]\ttrain-mlogloss:0.16419\teval-mlogloss:0.29606\n",
      "[205]\ttrain-mlogloss:0.16384\teval-mlogloss:0.29622\n",
      "[206]\ttrain-mlogloss:0.16363\teval-mlogloss:0.29609\n",
      "[207]\ttrain-mlogloss:0.16339\teval-mlogloss:0.29611\n",
      "[208]\ttrain-mlogloss:0.16314\teval-mlogloss:0.29600\n",
      "[209]\ttrain-mlogloss:0.16289\teval-mlogloss:0.29604\n",
      "[210]\ttrain-mlogloss:0.16268\teval-mlogloss:0.29601\n",
      "[211]\ttrain-mlogloss:0.16235\teval-mlogloss:0.29609\n",
      "[212]\ttrain-mlogloss:0.16209\teval-mlogloss:0.29632\n",
      "[213]\ttrain-mlogloss:0.16175\teval-mlogloss:0.29640\n",
      "[214]\ttrain-mlogloss:0.16147\teval-mlogloss:0.29609\n",
      "[215]\ttrain-mlogloss:0.16130\teval-mlogloss:0.29599\n",
      "[216]\ttrain-mlogloss:0.16096\teval-mlogloss:0.29604\n",
      "[217]\ttrain-mlogloss:0.16067\teval-mlogloss:0.29614\n",
      "[218]\ttrain-mlogloss:0.16036\teval-mlogloss:0.29613\n",
      "xgb now score is: [2.2251069462671875, 2.7018406702484934, 2.5559553388599308, 2.4211225976515562]\n",
      "[0]\ttrain-mlogloss:0.67136\teval-mlogloss:0.67073\n",
      "[1]\ttrain-mlogloss:0.65114\teval-mlogloss:0.64975\n",
      "[2]\ttrain-mlogloss:0.63203\teval-mlogloss:0.63032\n",
      "[3]\ttrain-mlogloss:0.61384\teval-mlogloss:0.61170\n",
      "[4]\ttrain-mlogloss:0.59670\teval-mlogloss:0.59410\n",
      "[5]\ttrain-mlogloss:0.58072\teval-mlogloss:0.57762\n",
      "[6]\ttrain-mlogloss:0.56522\teval-mlogloss:0.56158\n",
      "[7]\ttrain-mlogloss:0.55076\teval-mlogloss:0.54660\n",
      "[8]\ttrain-mlogloss:0.53704\teval-mlogloss:0.53230\n",
      "[9]\ttrain-mlogloss:0.52388\teval-mlogloss:0.51889\n",
      "[10]\ttrain-mlogloss:0.51128\teval-mlogloss:0.50609\n",
      "[11]\ttrain-mlogloss:0.49938\teval-mlogloss:0.49401\n",
      "[12]\ttrain-mlogloss:0.48806\teval-mlogloss:0.48229\n",
      "[13]\ttrain-mlogloss:0.47717\teval-mlogloss:0.47092\n",
      "[14]\ttrain-mlogloss:0.46678\teval-mlogloss:0.46006\n",
      "[15]\ttrain-mlogloss:0.45703\teval-mlogloss:0.45016\n",
      "[16]\ttrain-mlogloss:0.44733\teval-mlogloss:0.44019\n",
      "[17]\ttrain-mlogloss:0.43842\teval-mlogloss:0.43096\n",
      "[18]\ttrain-mlogloss:0.42978\teval-mlogloss:0.42189\n",
      "[19]\ttrain-mlogloss:0.42148\teval-mlogloss:0.41334\n",
      "[20]\ttrain-mlogloss:0.41365\teval-mlogloss:0.40521\n",
      "[21]\ttrain-mlogloss:0.40612\teval-mlogloss:0.39731\n",
      "[22]\ttrain-mlogloss:0.39878\teval-mlogloss:0.38965\n",
      "[23]\ttrain-mlogloss:0.39182\teval-mlogloss:0.38242\n",
      "[24]\ttrain-mlogloss:0.38536\teval-mlogloss:0.37591\n",
      "[25]\ttrain-mlogloss:0.37897\teval-mlogloss:0.36939\n",
      "[26]\ttrain-mlogloss:0.37305\teval-mlogloss:0.36335\n",
      "[27]\ttrain-mlogloss:0.36730\teval-mlogloss:0.35745\n",
      "[28]\ttrain-mlogloss:0.36168\teval-mlogloss:0.35157\n",
      "[29]\ttrain-mlogloss:0.35642\teval-mlogloss:0.34609\n",
      "[30]\ttrain-mlogloss:0.35121\teval-mlogloss:0.34074\n",
      "[31]\ttrain-mlogloss:0.34632\teval-mlogloss:0.33583\n",
      "[32]\ttrain-mlogloss:0.34162\teval-mlogloss:0.33115\n",
      "[33]\ttrain-mlogloss:0.33710\teval-mlogloss:0.32650\n",
      "[34]\ttrain-mlogloss:0.33271\teval-mlogloss:0.32193\n",
      "[35]\ttrain-mlogloss:0.32861\teval-mlogloss:0.31778\n",
      "[36]\ttrain-mlogloss:0.32462\teval-mlogloss:0.31382\n",
      "[37]\ttrain-mlogloss:0.32083\teval-mlogloss:0.30992\n",
      "[38]\ttrain-mlogloss:0.31718\teval-mlogloss:0.30621\n",
      "[39]\ttrain-mlogloss:0.31349\teval-mlogloss:0.30255\n",
      "[40]\ttrain-mlogloss:0.31014\teval-mlogloss:0.29905\n",
      "[41]\ttrain-mlogloss:0.30686\teval-mlogloss:0.29589\n",
      "[42]\ttrain-mlogloss:0.30349\teval-mlogloss:0.29290\n",
      "[43]\ttrain-mlogloss:0.30036\teval-mlogloss:0.28990\n",
      "[44]\ttrain-mlogloss:0.29733\teval-mlogloss:0.28688\n",
      "[45]\ttrain-mlogloss:0.29425\teval-mlogloss:0.28404\n",
      "[46]\ttrain-mlogloss:0.29150\teval-mlogloss:0.28126\n",
      "[47]\ttrain-mlogloss:0.28878\teval-mlogloss:0.27878\n",
      "[48]\ttrain-mlogloss:0.28627\teval-mlogloss:0.27635\n",
      "[49]\ttrain-mlogloss:0.28370\teval-mlogloss:0.27391\n",
      "[50]\ttrain-mlogloss:0.28126\teval-mlogloss:0.27155\n",
      "[51]\ttrain-mlogloss:0.27872\teval-mlogloss:0.26938\n",
      "[52]\ttrain-mlogloss:0.27637\teval-mlogloss:0.26757\n",
      "[53]\ttrain-mlogloss:0.27406\teval-mlogloss:0.26545\n",
      "[54]\ttrain-mlogloss:0.27196\teval-mlogloss:0.26337\n",
      "[55]\ttrain-mlogloss:0.26981\teval-mlogloss:0.26138\n",
      "[56]\ttrain-mlogloss:0.26777\teval-mlogloss:0.25967\n",
      "[57]\ttrain-mlogloss:0.26570\teval-mlogloss:0.25787\n",
      "[58]\ttrain-mlogloss:0.26401\teval-mlogloss:0.25622\n",
      "[59]\ttrain-mlogloss:0.26212\teval-mlogloss:0.25470\n",
      "[60]\ttrain-mlogloss:0.26039\teval-mlogloss:0.25321\n",
      "[61]\ttrain-mlogloss:0.25880\teval-mlogloss:0.25191\n",
      "[62]\ttrain-mlogloss:0.25723\teval-mlogloss:0.25047\n",
      "[63]\ttrain-mlogloss:0.25557\teval-mlogloss:0.24904\n",
      "[64]\ttrain-mlogloss:0.25414\teval-mlogloss:0.24784\n",
      "[65]\ttrain-mlogloss:0.25266\teval-mlogloss:0.24655\n",
      "[66]\ttrain-mlogloss:0.25129\teval-mlogloss:0.24542\n",
      "[67]\ttrain-mlogloss:0.24985\teval-mlogloss:0.24416\n",
      "[68]\ttrain-mlogloss:0.24840\teval-mlogloss:0.24307\n",
      "[69]\ttrain-mlogloss:0.24712\teval-mlogloss:0.24185\n",
      "[70]\ttrain-mlogloss:0.24578\teval-mlogloss:0.24075\n",
      "[71]\ttrain-mlogloss:0.24461\teval-mlogloss:0.23989\n",
      "[72]\ttrain-mlogloss:0.24336\teval-mlogloss:0.23891\n",
      "[73]\ttrain-mlogloss:0.24234\teval-mlogloss:0.23814\n",
      "[74]\ttrain-mlogloss:0.24103\teval-mlogloss:0.23724\n",
      "[75]\ttrain-mlogloss:0.23999\teval-mlogloss:0.23639\n",
      "[76]\ttrain-mlogloss:0.23881\teval-mlogloss:0.23559\n",
      "[77]\ttrain-mlogloss:0.23779\teval-mlogloss:0.23494\n",
      "[78]\ttrain-mlogloss:0.23661\teval-mlogloss:0.23431\n",
      "[79]\ttrain-mlogloss:0.23567\teval-mlogloss:0.23357\n",
      "[80]\ttrain-mlogloss:0.23466\teval-mlogloss:0.23289\n",
      "[81]\ttrain-mlogloss:0.23375\teval-mlogloss:0.23237\n",
      "[82]\ttrain-mlogloss:0.23278\teval-mlogloss:0.23182\n",
      "[83]\ttrain-mlogloss:0.23207\teval-mlogloss:0.23120\n",
      "[84]\ttrain-mlogloss:0.23115\teval-mlogloss:0.23072\n",
      "[85]\ttrain-mlogloss:0.23019\teval-mlogloss:0.23012\n",
      "[86]\ttrain-mlogloss:0.22908\teval-mlogloss:0.22971\n",
      "[87]\ttrain-mlogloss:0.22825\teval-mlogloss:0.22918\n",
      "[88]\ttrain-mlogloss:0.22734\teval-mlogloss:0.22869\n",
      "[89]\ttrain-mlogloss:0.22648\teval-mlogloss:0.22826\n",
      "[90]\ttrain-mlogloss:0.22555\teval-mlogloss:0.22767\n",
      "[91]\ttrain-mlogloss:0.22481\teval-mlogloss:0.22728\n",
      "[92]\ttrain-mlogloss:0.22401\teval-mlogloss:0.22697\n",
      "[93]\ttrain-mlogloss:0.22308\teval-mlogloss:0.22655\n",
      "[94]\ttrain-mlogloss:0.22226\teval-mlogloss:0.22621\n",
      "[95]\ttrain-mlogloss:0.22138\teval-mlogloss:0.22590\n",
      "[96]\ttrain-mlogloss:0.22073\teval-mlogloss:0.22568\n",
      "[97]\ttrain-mlogloss:0.21990\teval-mlogloss:0.22523\n",
      "[98]\ttrain-mlogloss:0.21933\teval-mlogloss:0.22483\n",
      "[99]\ttrain-mlogloss:0.21856\teval-mlogloss:0.22457\n",
      "[100]\ttrain-mlogloss:0.21793\teval-mlogloss:0.22432\n",
      "[101]\ttrain-mlogloss:0.21727\teval-mlogloss:0.22396\n",
      "[102]\ttrain-mlogloss:0.21669\teval-mlogloss:0.22368\n",
      "[103]\ttrain-mlogloss:0.21580\teval-mlogloss:0.22351\n",
      "[104]\ttrain-mlogloss:0.21505\teval-mlogloss:0.22342\n",
      "[105]\ttrain-mlogloss:0.21436\teval-mlogloss:0.22320\n",
      "[106]\ttrain-mlogloss:0.21362\teval-mlogloss:0.22305\n",
      "[107]\ttrain-mlogloss:0.21309\teval-mlogloss:0.22285\n",
      "[108]\ttrain-mlogloss:0.21234\teval-mlogloss:0.22276\n",
      "[109]\ttrain-mlogloss:0.21162\teval-mlogloss:0.22261\n",
      "[110]\ttrain-mlogloss:0.21095\teval-mlogloss:0.22239\n",
      "[111]\ttrain-mlogloss:0.21037\teval-mlogloss:0.22226\n",
      "[112]\ttrain-mlogloss:0.20980\teval-mlogloss:0.22225\n",
      "[113]\ttrain-mlogloss:0.20916\teval-mlogloss:0.22211\n",
      "[114]\ttrain-mlogloss:0.20860\teval-mlogloss:0.22204\n",
      "[115]\ttrain-mlogloss:0.20808\teval-mlogloss:0.22175\n",
      "[116]\ttrain-mlogloss:0.20738\teval-mlogloss:0.22160\n",
      "[117]\ttrain-mlogloss:0.20684\teval-mlogloss:0.22143\n",
      "[118]\ttrain-mlogloss:0.20627\teval-mlogloss:0.22129\n",
      "[119]\ttrain-mlogloss:0.20582\teval-mlogloss:0.22108\n",
      "[120]\ttrain-mlogloss:0.20533\teval-mlogloss:0.22109\n",
      "[121]\ttrain-mlogloss:0.20479\teval-mlogloss:0.22089\n",
      "[122]\ttrain-mlogloss:0.20439\teval-mlogloss:0.22067\n",
      "[123]\ttrain-mlogloss:0.20375\teval-mlogloss:0.22069\n",
      "[124]\ttrain-mlogloss:0.20319\teval-mlogloss:0.22068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125]\ttrain-mlogloss:0.20252\teval-mlogloss:0.22084\n",
      "[126]\ttrain-mlogloss:0.20197\teval-mlogloss:0.22077\n",
      "[127]\ttrain-mlogloss:0.20139\teval-mlogloss:0.22073\n",
      "[128]\ttrain-mlogloss:0.20106\teval-mlogloss:0.22061\n",
      "[129]\ttrain-mlogloss:0.20033\teval-mlogloss:0.22065\n",
      "[130]\ttrain-mlogloss:0.19982\teval-mlogloss:0.22052\n",
      "[131]\ttrain-mlogloss:0.19931\teval-mlogloss:0.22048\n",
      "[132]\ttrain-mlogloss:0.19880\teval-mlogloss:0.22031\n",
      "[133]\ttrain-mlogloss:0.19834\teval-mlogloss:0.22027\n",
      "[134]\ttrain-mlogloss:0.19794\teval-mlogloss:0.22008\n",
      "[135]\ttrain-mlogloss:0.19753\teval-mlogloss:0.22008\n",
      "[136]\ttrain-mlogloss:0.19688\teval-mlogloss:0.21997\n",
      "[137]\ttrain-mlogloss:0.19645\teval-mlogloss:0.21999\n",
      "[138]\ttrain-mlogloss:0.19600\teval-mlogloss:0.22003\n",
      "[139]\ttrain-mlogloss:0.19554\teval-mlogloss:0.21983\n",
      "[140]\ttrain-mlogloss:0.19505\teval-mlogloss:0.21978\n",
      "[141]\ttrain-mlogloss:0.19453\teval-mlogloss:0.21988\n",
      "[142]\ttrain-mlogloss:0.19409\teval-mlogloss:0.21966\n",
      "[143]\ttrain-mlogloss:0.19372\teval-mlogloss:0.21965\n",
      "[144]\ttrain-mlogloss:0.19312\teval-mlogloss:0.21965\n",
      "[145]\ttrain-mlogloss:0.19245\teval-mlogloss:0.21989\n",
      "[146]\ttrain-mlogloss:0.19204\teval-mlogloss:0.21986\n",
      "[147]\ttrain-mlogloss:0.19159\teval-mlogloss:0.21988\n",
      "[148]\ttrain-mlogloss:0.19113\teval-mlogloss:0.22000\n",
      "[149]\ttrain-mlogloss:0.19067\teval-mlogloss:0.22010\n",
      "[150]\ttrain-mlogloss:0.19018\teval-mlogloss:0.21987\n",
      "[151]\ttrain-mlogloss:0.18965\teval-mlogloss:0.21999\n",
      "[152]\ttrain-mlogloss:0.18931\teval-mlogloss:0.21993\n",
      "[153]\ttrain-mlogloss:0.18894\teval-mlogloss:0.22002\n",
      "[154]\ttrain-mlogloss:0.18848\teval-mlogloss:0.21987\n",
      "[155]\ttrain-mlogloss:0.18794\teval-mlogloss:0.21996\n",
      "[156]\ttrain-mlogloss:0.18766\teval-mlogloss:0.21992\n",
      "[157]\ttrain-mlogloss:0.18732\teval-mlogloss:0.21990\n",
      "[158]\ttrain-mlogloss:0.18685\teval-mlogloss:0.21990\n",
      "[159]\ttrain-mlogloss:0.18641\teval-mlogloss:0.22002\n",
      "[160]\ttrain-mlogloss:0.18594\teval-mlogloss:0.22012\n",
      "[161]\ttrain-mlogloss:0.18558\teval-mlogloss:0.22009\n",
      "[162]\ttrain-mlogloss:0.18523\teval-mlogloss:0.22018\n",
      "[163]\ttrain-mlogloss:0.18490\teval-mlogloss:0.22033\n",
      "[164]\ttrain-mlogloss:0.18457\teval-mlogloss:0.22025\n",
      "[165]\ttrain-mlogloss:0.18421\teval-mlogloss:0.22017\n",
      "[166]\ttrain-mlogloss:0.18390\teval-mlogloss:0.22037\n",
      "[167]\ttrain-mlogloss:0.18350\teval-mlogloss:0.22039\n",
      "[168]\ttrain-mlogloss:0.18310\teval-mlogloss:0.22050\n",
      "[169]\ttrain-mlogloss:0.18271\teval-mlogloss:0.22069\n",
      "[170]\ttrain-mlogloss:0.18235\teval-mlogloss:0.22076\n",
      "[171]\ttrain-mlogloss:0.18190\teval-mlogloss:0.22080\n",
      "[172]\ttrain-mlogloss:0.18153\teval-mlogloss:0.22086\n",
      "[173]\ttrain-mlogloss:0.18118\teval-mlogloss:0.22085\n",
      "[174]\ttrain-mlogloss:0.18073\teval-mlogloss:0.22068\n",
      "[175]\ttrain-mlogloss:0.18045\teval-mlogloss:0.22081\n",
      "[176]\ttrain-mlogloss:0.18011\teval-mlogloss:0.22069\n",
      "[177]\ttrain-mlogloss:0.17976\teval-mlogloss:0.22079\n",
      "[178]\ttrain-mlogloss:0.17937\teval-mlogloss:0.22092\n",
      "[179]\ttrain-mlogloss:0.17910\teval-mlogloss:0.22097\n",
      "[180]\ttrain-mlogloss:0.17879\teval-mlogloss:0.22100\n",
      "[181]\ttrain-mlogloss:0.17848\teval-mlogloss:0.22105\n",
      "[182]\ttrain-mlogloss:0.17808\teval-mlogloss:0.22091\n",
      "[183]\ttrain-mlogloss:0.17768\teval-mlogloss:0.22102\n",
      "[184]\ttrain-mlogloss:0.17731\teval-mlogloss:0.22109\n",
      "[185]\ttrain-mlogloss:0.17700\teval-mlogloss:0.22109\n",
      "[186]\ttrain-mlogloss:0.17673\teval-mlogloss:0.22105\n",
      "[187]\ttrain-mlogloss:0.17639\teval-mlogloss:0.22119\n",
      "[188]\ttrain-mlogloss:0.17598\teval-mlogloss:0.22112\n",
      "[189]\ttrain-mlogloss:0.17577\teval-mlogloss:0.22129\n",
      "[190]\ttrain-mlogloss:0.17545\teval-mlogloss:0.22141\n",
      "[191]\ttrain-mlogloss:0.17515\teval-mlogloss:0.22148\n",
      "[192]\ttrain-mlogloss:0.17494\teval-mlogloss:0.22160\n",
      "[193]\ttrain-mlogloss:0.17461\teval-mlogloss:0.22168\n",
      "[194]\ttrain-mlogloss:0.17440\teval-mlogloss:0.22167\n",
      "[195]\ttrain-mlogloss:0.17418\teval-mlogloss:0.22176\n",
      "[196]\ttrain-mlogloss:0.17383\teval-mlogloss:0.22146\n",
      "[197]\ttrain-mlogloss:0.17339\teval-mlogloss:0.22136\n",
      "[198]\ttrain-mlogloss:0.17303\teval-mlogloss:0.22106\n",
      "[199]\ttrain-mlogloss:0.17266\teval-mlogloss:0.22104\n",
      "[200]\ttrain-mlogloss:0.17239\teval-mlogloss:0.22094\n",
      "[201]\ttrain-mlogloss:0.17214\teval-mlogloss:0.22099\n",
      "[202]\ttrain-mlogloss:0.17183\teval-mlogloss:0.22107\n",
      "[203]\ttrain-mlogloss:0.17157\teval-mlogloss:0.22099\n",
      "[204]\ttrain-mlogloss:0.17122\teval-mlogloss:0.22109\n",
      "[205]\ttrain-mlogloss:0.17096\teval-mlogloss:0.22121\n",
      "[206]\ttrain-mlogloss:0.17065\teval-mlogloss:0.22129\n",
      "[207]\ttrain-mlogloss:0.17036\teval-mlogloss:0.22133\n",
      "[208]\ttrain-mlogloss:0.17016\teval-mlogloss:0.22126\n",
      "[209]\ttrain-mlogloss:0.16990\teval-mlogloss:0.22119\n",
      "[210]\ttrain-mlogloss:0.16962\teval-mlogloss:0.22112\n",
      "[211]\ttrain-mlogloss:0.16932\teval-mlogloss:0.22131\n",
      "[212]\ttrain-mlogloss:0.16908\teval-mlogloss:0.22153\n",
      "[213]\ttrain-mlogloss:0.16878\teval-mlogloss:0.22155\n",
      "[214]\ttrain-mlogloss:0.16848\teval-mlogloss:0.22161\n",
      "[215]\ttrain-mlogloss:0.16820\teval-mlogloss:0.22163\n",
      "[216]\ttrain-mlogloss:0.16793\teval-mlogloss:0.22157\n",
      "[217]\ttrain-mlogloss:0.16774\teval-mlogloss:0.22164\n",
      "[218]\ttrain-mlogloss:0.16744\teval-mlogloss:0.22174\n",
      "[219]\ttrain-mlogloss:0.16723\teval-mlogloss:0.22176\n",
      "[220]\ttrain-mlogloss:0.16703\teval-mlogloss:0.22169\n",
      "[221]\ttrain-mlogloss:0.16681\teval-mlogloss:0.22177\n",
      "[222]\ttrain-mlogloss:0.16668\teval-mlogloss:0.22177\n",
      "[223]\ttrain-mlogloss:0.16643\teval-mlogloss:0.22192\n",
      "[224]\ttrain-mlogloss:0.16608\teval-mlogloss:0.22173\n",
      "[225]\ttrain-mlogloss:0.16577\teval-mlogloss:0.22172\n",
      "[226]\ttrain-mlogloss:0.16542\teval-mlogloss:0.22176\n",
      "[227]\ttrain-mlogloss:0.16512\teval-mlogloss:0.22178\n",
      "[228]\ttrain-mlogloss:0.16484\teval-mlogloss:0.22193\n",
      "[229]\ttrain-mlogloss:0.16457\teval-mlogloss:0.22187\n",
      "[230]\ttrain-mlogloss:0.16439\teval-mlogloss:0.22193\n",
      "[231]\ttrain-mlogloss:0.16426\teval-mlogloss:0.22202\n",
      "[232]\ttrain-mlogloss:0.16402\teval-mlogloss:0.22187\n",
      "[233]\ttrain-mlogloss:0.16392\teval-mlogloss:0.22198\n",
      "[234]\ttrain-mlogloss:0.16364\teval-mlogloss:0.22188\n",
      "[235]\ttrain-mlogloss:0.16347\teval-mlogloss:0.22199\n",
      "[236]\ttrain-mlogloss:0.16336\teval-mlogloss:0.22201\n",
      "[237]\ttrain-mlogloss:0.16313\teval-mlogloss:0.22193\n",
      "[238]\ttrain-mlogloss:0.16290\teval-mlogloss:0.22197\n",
      "[239]\ttrain-mlogloss:0.16259\teval-mlogloss:0.22212\n",
      "[240]\ttrain-mlogloss:0.16229\teval-mlogloss:0.22203\n",
      "[241]\ttrain-mlogloss:0.16209\teval-mlogloss:0.22210\n",
      "[242]\ttrain-mlogloss:0.16188\teval-mlogloss:0.22222\n",
      "[243]\ttrain-mlogloss:0.16159\teval-mlogloss:0.22246\n",
      "xgb now score is: [2.2251069462671875, 2.7018406702484934, 2.5559553388599308, 2.4211225976515562, 2.586664668461308]\n",
      "xgb_score_list: [2.2251069462671875, 2.7018406702484934, 2.5559553388599308, 2.4211225976515562, 2.586664668461308]\n",
      "xgb_score_mean: 2.4981380442976953\n"
     ]
    }
   ],
   "source": [
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65135322",
   "metadata": {},
   "source": [
    "## 原始特征和stacking特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c6184d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88773735",
   "metadata": {},
   "source": [
    "## 特征重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a41ee2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d954e",
   "metadata": {},
   "source": [
    "## 获取数据ID以及特征标签LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b22a165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['label'] = all_data_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d1517",
   "metadata": {},
   "source": [
    "## 训练数据和测试数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff872841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('test_all.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "93799b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>embeeding_93</th>\n",
       "      <th>embeeding_94</th>\n",
       "      <th>embeeding_95</th>\n",
       "      <th>embeeding_96</th>\n",
       "      <th>embeeding_97</th>\n",
       "      <th>embeeding_98</th>\n",
       "      <th>embeeding_99</th>\n",
       "      <th>lgb_clf</th>\n",
       "      <th>xgb_clf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600.0</td>\n",
       "      <td>1487.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.940233</td>\n",
       "      <td>0.903583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948859</td>\n",
       "      <td>0.915382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910761</td>\n",
       "      <td>0.945769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.924750</td>\n",
       "      <td>0.928050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933608</td>\n",
       "      <td>0.931714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>305721.0</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942053</td>\n",
       "      <td>0.957956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>109881.0</td>\n",
       "      <td>2639.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930127</td>\n",
       "      <td>0.915183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>185145.0</td>\n",
       "      <td>4950.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.929671</td>\n",
       "      <td>0.926973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>131385.0</td>\n",
       "      <td>1582.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930781</td>\n",
       "      <td>0.956378</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>7737.0</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932701</td>\n",
       "      <td>0.915894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  merchant_id  age_range  gender  user_cnt  seller_nunique  \\\n",
       "0     105600.0       1487.0        6.0     1.0     310.0            96.0   \n",
       "1     110976.0        159.0        5.0     0.0     274.0           181.0   \n",
       "2     374400.0        302.0        5.0     1.0     278.0            57.0   \n",
       "3     189312.0       1760.0        4.0     0.0     237.0            49.0   \n",
       "4     189312.0       2511.0        4.0     0.0     237.0            49.0   \n",
       "...        ...          ...        ...     ...       ...             ...   \n",
       "1995  305721.0       3734.0        3.0     1.0      35.0            13.0   \n",
       "1996  109881.0       2639.0        4.0     0.0     284.0            51.0   \n",
       "1997  185145.0       4950.0        4.0     1.0      84.0            31.0   \n",
       "1998  131385.0       1582.0        3.0     1.0      92.0            38.0   \n",
       "1999    7737.0       2066.0        4.0     0.0     210.0            49.0   \n",
       "\n",
       "      cat_nunique  brand_nunique  item_nunique  time_stamp_nunique  ...  \\\n",
       "0            37.0           88.0         217.0                29.0  ...   \n",
       "1            70.0          159.0         233.0                52.0  ...   \n",
       "2            59.0           62.0         148.0                35.0  ...   \n",
       "3            35.0           45.0         170.0                 9.0  ...   \n",
       "4            35.0           45.0         170.0                 9.0  ...   \n",
       "...           ...            ...           ...                 ...  ...   \n",
       "1995          7.0           16.0          21.0                 5.0  ...   \n",
       "1996         55.0           53.0         186.0                32.0  ...   \n",
       "1997         34.0           33.0          58.0                27.0  ...   \n",
       "1998         18.0           36.0          63.0                15.0  ...   \n",
       "1999         36.0           54.0         112.0                35.0  ...   \n",
       "\n",
       "      embeeding_93  embeeding_94  embeeding_95  embeeding_96  embeeding_97  \\\n",
       "0              0.0           0.0           0.0           0.0           0.0   \n",
       "1              0.0           0.0           0.0           0.0           0.0   \n",
       "2              0.0           0.0           0.0           0.0           0.0   \n",
       "3              0.0           0.0           0.0           0.0           0.0   \n",
       "4              0.0           0.0           0.0           0.0           0.0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995           0.0           0.0           0.0           0.0           0.0   \n",
       "1996           0.0           0.0           0.0           0.0           0.0   \n",
       "1997           0.0           0.0           0.0           0.0           0.0   \n",
       "1998           0.0           0.0           0.0           0.0           0.0   \n",
       "1999           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      embeeding_98  embeeding_99   lgb_clf   xgb_clf  label  \n",
       "0              0.0           0.0  0.940233  0.903583    0.0  \n",
       "1              0.0           0.0  0.948859  0.915382    0.0  \n",
       "2              0.0           0.0  0.910761  0.945769    0.0  \n",
       "3              0.0           0.0  0.924750  0.928050    0.0  \n",
       "4              0.0           0.0  0.933608  0.931714    0.0  \n",
       "...            ...           ...       ...       ...    ...  \n",
       "1995           0.0           0.0  0.942053  0.957956    0.0  \n",
       "1996           0.0           0.0  0.930127  0.915183    0.0  \n",
       "1997           0.0           0.0  0.929671  0.926973    0.0  \n",
       "1998           0.0           0.0  0.930781  0.956378    0.0  \n",
       "1999           0.0           0.0  0.932701  0.915894    0.0  \n",
       "\n",
       "[2000 rows x 234 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ba7aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>embeeding_92</th>\n",
       "      <th>embeeding_93</th>\n",
       "      <th>embeeding_94</th>\n",
       "      <th>embeeding_95</th>\n",
       "      <th>embeeding_96</th>\n",
       "      <th>embeeding_97</th>\n",
       "      <th>embeeding_98</th>\n",
       "      <th>embeeding_99</th>\n",
       "      <th>lgb_clf</th>\n",
       "      <th>xgb_clf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600.0</td>\n",
       "      <td>1487.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934078</td>\n",
       "      <td>0.856887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.939018</td>\n",
       "      <td>0.941427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.927639</td>\n",
       "      <td>0.935769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934111</td>\n",
       "      <td>0.931642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932230</td>\n",
       "      <td>0.927470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>305721.0</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.939274</td>\n",
       "      <td>0.940982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>109881.0</td>\n",
       "      <td>2639.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925575</td>\n",
       "      <td>0.897880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>185145.0</td>\n",
       "      <td>4950.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941051</td>\n",
       "      <td>0.938261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>131385.0</td>\n",
       "      <td>1582.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.945235</td>\n",
       "      <td>0.955478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>7737.0</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.924899</td>\n",
       "      <td>0.919442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  merchant_id  age_range  gender  user_cnt  seller_nunique  \\\n",
       "0     105600.0       1487.0        6.0     1.0     310.0            96.0   \n",
       "1     110976.0        159.0        5.0     0.0     274.0           181.0   \n",
       "2     374400.0        302.0        5.0     1.0     278.0            57.0   \n",
       "3     189312.0       1760.0        4.0     0.0     237.0            49.0   \n",
       "4     189312.0       2511.0        4.0     0.0     237.0            49.0   \n",
       "...        ...          ...        ...     ...       ...             ...   \n",
       "1995  305721.0       3734.0        3.0     1.0      35.0            13.0   \n",
       "1996  109881.0       2639.0        4.0     0.0     284.0            51.0   \n",
       "1997  185145.0       4950.0        4.0     1.0      84.0            31.0   \n",
       "1998  131385.0       1582.0        3.0     1.0      92.0            38.0   \n",
       "1999    7737.0       2066.0        4.0     0.0     210.0            49.0   \n",
       "\n",
       "      cat_nunique  brand_nunique  item_nunique  time_stamp_nunique  ...  \\\n",
       "0            37.0           88.0         217.0                29.0  ...   \n",
       "1            70.0          159.0         233.0                52.0  ...   \n",
       "2            59.0           62.0         148.0                35.0  ...   \n",
       "3            35.0           45.0         170.0                 9.0  ...   \n",
       "4            35.0           45.0         170.0                 9.0  ...   \n",
       "...           ...            ...           ...                 ...  ...   \n",
       "1995          7.0           16.0          21.0                 5.0  ...   \n",
       "1996         55.0           53.0         186.0                32.0  ...   \n",
       "1997         34.0           33.0          58.0                27.0  ...   \n",
       "1998         18.0           36.0          63.0                15.0  ...   \n",
       "1999         36.0           54.0         112.0                35.0  ...   \n",
       "\n",
       "      embeeding_92  embeeding_93  embeeding_94  embeeding_95  embeeding_96  \\\n",
       "0              0.0           0.0           0.0           0.0           0.0   \n",
       "1              0.0           0.0           0.0           0.0           0.0   \n",
       "2              0.0           0.0           0.0           0.0           0.0   \n",
       "3              0.0           0.0           0.0           0.0           0.0   \n",
       "4              0.0           0.0           0.0           0.0           0.0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995           0.0           0.0           0.0           0.0           0.0   \n",
       "1996           0.0           0.0           0.0           0.0           0.0   \n",
       "1997           0.0           0.0           0.0           0.0           0.0   \n",
       "1998           0.0           0.0           0.0           0.0           0.0   \n",
       "1999           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      embeeding_97  embeeding_98  embeeding_99   lgb_clf   xgb_clf  \n",
       "0              0.0           0.0           0.0  0.934078  0.856887  \n",
       "1              0.0           0.0           0.0  0.939018  0.941427  \n",
       "2              0.0           0.0           0.0  0.927639  0.935769  \n",
       "3              0.0           0.0           0.0  0.934111  0.931642  \n",
       "4              0.0           0.0           0.0  0.932230  0.927470  \n",
       "...            ...           ...           ...       ...       ...  \n",
       "1995           0.0           0.0           0.0  0.939274  0.940982  \n",
       "1996           0.0           0.0           0.0  0.925575  0.897880  \n",
       "1997           0.0           0.0           0.0  0.941051  0.938261  \n",
       "1998           0.0           0.0           0.0  0.945235  0.955478  \n",
       "1999           0.0           0.0           0.0  0.924899  0.919442  \n",
       "\n",
       "[2000 rows x 233 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
